[
  {
    "objectID": "Personal_Proj/LISA/Take-Home_Ex1.html",
    "href": "Personal_Proj/LISA/Take-Home_Ex1.html",
    "title": "Analysis of Geospatial Distribution of Bus Ridership by Origin Bus Stop in Singapore",
    "section": "",
    "text": "Main tool/language: R"
  },
  {
    "objectID": "Personal_Proj/LISA/Take-Home_Ex1.html#objectives",
    "href": "Personal_Proj/LISA/Take-Home_Ex1.html#objectives",
    "title": "Analysis of Geospatial Distribution of Bus Ridership by Origin Bus Stop in Singapore",
    "section": "Objectives",
    "text": "Objectives\nThe bus system is one of Singapore’s two pillars of public transport aside from the MRT. The bus system ensures convenient and affordable short-, medium-, and long-distance travel for riders. Thanks to the widespread availability of bus stops as compared to MRT stations, it has a high level of accessibility. However, this also leaves the system prone to under- or over-investment in terms of the number of bus routes, leading some stops and routes to be under-served or over-served.\nThe objective of this study is to examine the distribution of bus trips in Singapore by analyzing the number of trips by originating bus stops. It will consist of two levels of analysis:\n\nGeoVisualisation and Analysis: Visualizing the number of trips by originating bus stops and provide descriptive statistics of the distribution of trips by bus stops.\nLocal Indicators of Spatial Association Analysis (LISA): This analysis involves the calculation of Local Moran’s I to determine local spatial autocorrelation between a bus stop and its neighbors. Additionally, visualizations such as a LISA cluster map will be created for easier comparison."
  },
  {
    "objectID": "Personal_Proj/LISA/Take-Home_Ex1.html#getting-started",
    "href": "Personal_Proj/LISA/Take-Home_Ex1.html#getting-started",
    "title": "Analysis of Geospatial Distribution of Bus Ridership by Origin Bus Stop in Singapore",
    "section": "Getting Started",
    "text": "Getting Started\nFirst, the necessary R packages will be loaded using the p_load() function of the pacman package. p_load() will also install any package which is not already installed. The following packages will be loaded:\n\nsf: For handling of geospatial data.\nsfdep: For determining the spatial dependence of spatial features. The three main categories of functionality relates to the determination of geometry neighbors, weights, and LISA.\ntidyverse: For manipulation of non-spatial data. This package contains ggplot2 for plotting, dplyr and tidyr for dataframe manipulation, and readr for reading comma-separated values (CSV).\ntmap: For thematic mapping, especially the mapping of simple features data frame.\nspdep: For drawing Moran scatterplot.\n\n\npacman::p_load(sf,sfdep,tidyverse,tmap, spdep)"
  },
  {
    "objectID": "Personal_Proj/LISA/Take-Home_Ex1.html#importing-required-data",
    "href": "Personal_Proj/LISA/Take-Home_Ex1.html#importing-required-data",
    "title": "Analysis of Geospatial Distribution of Bus Ridership by Origin Bus Stop in Singapore",
    "section": "Importing Required Data",
    "text": "Importing Required Data\nFor the purpose of this study, two types of data will be used: geospatial data which consists of spatial features and their coordinates information, and aspatial data which consists of attributes which can be ascribed to the geospatial data. Specifically, the following datasets will be used for each type:\n\nGeospatial Data:\n\nBusStop.shp: This shape file contains the location of the bus stops in Singapore as at July 2023. This file can be retrieved from the Land Transport Authority (LTA) Data Mall (link).\n\nAspatial Data:\n\norigin_destination_bus_202309.csv: This CSV file contains the detail of bus trips from an originating bus stop to a destination bus stop, identified by their unique codes, each hour of the day during September 2023. The data is further broken down into weekend or weekday, but not by the specific day of the week. This data can be retrieved by using the LTA Data Mall’s API (link).\n\n\nThe first steps taken will be to import these files into the R environment in a manipulable format.\n\nImporting Geospatial Data\nGeospatial data can be imported using the st_read() function of the sf package. This will import the file into the R environment as a sf (simple features) data frame. st_transform() is added to transform the Coordinate Reference System (CRS) to EPSG: 3414, which is the CRS of Singapore.\n\n\n\n\n\n\nNote\n\n\n\nIn st_read():\n\ndsn: the directory where the shape file is stored\nlayer: the name of the shape file\n\n\n\n\nbusstop &lt;- st_read(dsn = 'data/geospatial',\n                   layer = 'BusStop') %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `D:\\phlong2023\\Personal_Projects\\Personal_Proj\\LISA\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nFrom the message provided by R, it can be seen that the busstop sf data frame has 5161 rows, 3 columns, and has a CRS of SVY 21.\nTo get a better grasp of the busstop data frame, glimpse() function can be used.\n\n\n\n\n\n\nNote\n\n\n\nThe data type for each column can be seen as well as some of their values. For sf data frames, there is a geometry column (POINT type) which contains the location information for each polygon.\n\n\n\nglimpse(busstop)\n\nRows: 5,161\nColumns: 4\n$ BUS_STOP_N &lt;chr&gt; \"22069\", \"32071\", \"44331\", \"96081\", \"11561\", \"66191\", \"2338…\n$ BUS_ROOF_N &lt;chr&gt; \"B06\", \"B23\", \"B01\", \"B05\", \"B05\", \"B03\", \"B02A\", \"B02\", \"B…\n$ LOC_DESC   &lt;chr&gt; \"OPP CEVA LOGISTICS\", \"AFT TRACK 13\", \"BLK 239\", \"GRACE IND…\n$ geometry   &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.38),…\n\n\nAdditionally, busstop can be visualized in order to spot any anomaly. This can be done using the qtm() function in the tmap package for quick plotting.\n\nqtm(busstop)\n\n\n\n\nThe visualization shows us that there are four bus stops in Malaysia. Let’s remove them so that only bus stops in Singapore will be considered. This is because these special bus stops might exhibit different behaviors due to their different context from the rest of the bus stops in Singapore.\nfilter() can be used in conjunction with a dplyr step to remove these bus stops.\n\nbusstop &lt;- busstop %&gt;%\n  filter(!BUS_STOP_N %in% c('46609','47701', '46211', '46219', '46239'))\n\n\n\n\n\n\n\nNote\n\n\n\nqtm() can be used again to check that the bus stops have been removed.\n\n\n\n\nImporting Aspatial Data\nThe read_csv() function of readr can be used to import the origin_destination_bus_202309 CSV file into the R environment as a data frame.\n\npassenger &lt;- read_csv('data/aspatial/origin_destination_bus_202309.csv')\n\nFrom the message provided by R, it can be seen that the passenger has 5,714,196 rows and 7 columns.\nhead() can be used instead of glimpse() to view the top five rows of the passenger data frame. This will also allow us to see the data type of each of the column.\n\nhead(passenger)\n\n# A tibble: 6 × 7\n  YEAR_MONTH DAY_TYPE   TIME_PER_HOUR PT_TYPE ORIGIN_PT_CODE DESTINATION_PT_CODE\n  &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;              \n1 2023-09    WEEKENDS/…            17 BUS     24499          22221              \n2 2023-09    WEEKENDS/…            10 BUS     65239          65159              \n3 2023-09    WEEKDAY               10 BUS     65239          65159              \n4 2023-09    WEEKDAY                7 BUS     23519          23311              \n5 2023-09    WEEKENDS/…             7 BUS     23519          23311              \n6 2023-09    WEEKENDS/…            11 BUS     52509          42041              \n# ℹ 1 more variable: TOTAL_TRIPS &lt;dbl&gt;\n\n\nNote that the ORIGIN_PT_CODE and DESTINATION_PT_CODE are in the character (“chr”) data type. However, we would like it to be in the factor (“fctr”) data type for easier categorization and sorting. This can be done by using the as.factor() function.\n\npassenger$ORIGIN_PT_CODE &lt;- as.factor(passenger$ORIGIN_PT_CODE)\npassenger$DESTINATION_PT_CODE &lt;- as.factor(passenger$DESTINATION_PT_CODE)\n\nWe can use head() to check the data type of the passenger data frame.\n\nhead(passenger)\n\n# A tibble: 6 × 7\n  YEAR_MONTH DAY_TYPE   TIME_PER_HOUR PT_TYPE ORIGIN_PT_CODE DESTINATION_PT_CODE\n  &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;fct&gt;          &lt;fct&gt;              \n1 2023-09    WEEKENDS/…            17 BUS     24499          22221              \n2 2023-09    WEEKENDS/…            10 BUS     65239          65159              \n3 2023-09    WEEKDAY               10 BUS     65239          65159              \n4 2023-09    WEEKDAY                7 BUS     23519          23311              \n5 2023-09    WEEKENDS/…             7 BUS     23519          23311              \n6 2023-09    WEEKENDS/…            11 BUS     52509          42041              \n# ℹ 1 more variable: TOTAL_TRIPS &lt;dbl&gt;"
  },
  {
    "objectID": "Personal_Proj/LISA/Take-Home_Ex1.html#data-preparation",
    "href": "Personal_Proj/LISA/Take-Home_Ex1.html#data-preparation",
    "title": "Analysis of Geospatial Distribution of Bus Ridership by Origin Bus Stop in Singapore",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn order to perform our analysis, certain manipulations must be made in order to prepare the data. Specifically, the passenger data set will be filtered and summarzied. Subsequently, it will be combined with the busstop data set based on the bus stop code variable present in both data frames.\n\nWrangling Aspatial Data\n\nFiltering the passenger Data Set for Desired Time Frames\nFor the purpose of this study, the passenger data set needs to be filtered to only contain trips falling within one of the following time frames:\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nThis can be accomplished using the filter() function and the dplyr steps. We can create four separate data frames to store the four different time frames\n\n# Weekday morning peak 6am - 9am\npassenger_wd_69 &lt;- passenger %&gt;%\n  filter(DAY_TYPE == 'WEEKDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 & TIME_PER_HOUR &lt;= 9)\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\npassenger_wd_1720 &lt;- passenger %&gt;%\n  filter(DAY_TYPE == 'WEEKDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 17 & TIME_PER_HOUR &lt;= 20)\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\npassenger_weh_1114 &lt;- passenger %&gt;%\n  filter(DAY_TYPE == 'WEEKENDS/HOLIDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 11 & TIME_PER_HOUR &lt;= 14)\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\npassenger_weh_1619 &lt;- passenger %&gt;%\n  filter(DAY_TYPE == 'WEEKENDS/HOLIDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 & TIME_PER_HOUR &lt;= 19)\n\nAfter the different trips have been categorized into their separate data frames, the total number trips for each origin bus stop can be tallied into a single statistic for the study period. This can be accomplished using the summarize() function. The example below shows this operation using passenger_wd_69.\n\n\n\n\n\n\nNote\n\n\n\nThe group_by() function is used to instruct R to conduct operations based on the groups created by group_by(). In this case, the summary operations will be done based on the origin bus stop codes.\n\n\n\n# Tallying the trips by origin bus stop for Weekday morning peak 6am - 9am\npassenger_wd_69_tallied &lt;- passenger_wd_69 %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\npassenger_wd_69_tallied\n\n# A tibble: 5,020 × 2\n   ORIGIN_PT_CODE TRIPS\n   &lt;fct&gt;          &lt;dbl&gt;\n 1 01012           1640\n 2 01013            764\n 3 01019           1322\n 4 01029           2373\n 5 01039           2562\n 6 01059           1582\n 7 01109            144\n 8 01112           7993\n 9 01113           6734\n10 01119           3736\n# ℹ 5,010 more rows\n\n\nAs can be seen, the newly created data frame consists only of the total trip numbers for each origin bus stop. This can be repeated for the other time frames.\n\n# Tallying the trips by origin bus stop for Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\npassenger_wd_1720_tallied &lt;- passenger_wd_1720 %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n# Tallying the trips by origin bus stop for Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\npassenger_weh_1114_tallied &lt;- passenger_weh_1114 %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n# Tallying the trips by origin bus stop for Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\npassenger_weh_1619_tallied &lt;- passenger_weh_1619 %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\n\n\nWrangling Geospatial Data\nIn order to adequately visualize the busstop sf data frame, we need to define a mapping layer. An example of a mapping layer would be to use the Master Plan 2019 Planning Sub-zone created by the Urban Redevelopment Authority (URA). However, for the purpose of this study, a hexagon layer will be used to ensure standardization of the size of each polygon and the evenly spaced gaps between a polygon and its neighbors.\nThe steps in this section will detail the creation of the hexagon layer using the busstop data frame and visualize the layer on a map of Singapore.\n\n\nCreating a Hexagon Layer in R\nThe steps taken in this section is based on the guide provided by Kenneth Wong of Urban Data Palette (link).\nFirstly, a hexagon or honeycomb grid can be created based on the busstop data frame using the st_make_grid() function.\n\n\n\n\n\n\nNote\n\n\n\nThere are some notable arguments in the st_make_grid() function:\n\ncellsize = c(500,500): This argument indicates the size of each hexagon, calculated as the distance between opposite edges. If the cell size is large, each hexagon can encompasses multiple bus stops, whereas if a smaller cell size can help us differentiate between individual bus stop. However, a smaller cell size with many hexagons will take more time to create. For this study, hexagons of 250m (this distance is the perpendicular distance between the centre of the hexagon and its edges) will be created with the parameter of 500.\nwhat = ‘polygons’: We would like to create polygons on a grid.\nsquare = FALSE: The default argument is TRUE, which would create a square grid. FALSE is specified in order to create a hexagon grid.\n\n\n\n\narea_honeycomb_grid = st_make_grid(busstop, cellsize = c(500,500), what = 'polygons', square = FALSE)\n\narea_honeycomb_grid\n\nGeometry set for 5040 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 3470.122 ymin: 26193.43 xmax: 48720.12 ymax: 50586.47\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nThe area_honeycomb_grid contains 136906 features of the same Projected CRS as the busstop data frame. If the plot() function is used, the hexagon grid will be displayed. However, this grid contains no information and might be too small to discern the individual cell.\n\n#qtm(area_honeycomb_grid)\n\nThe area_honey_comb needs to be converted to a sf data frame for further manipulation using st_sf(). Additionally, we can assign a unique id to each of the hexagon cell in area_honey_comb using mutate().\n\nhoneycomb_grid_sf = st_sf(area_honeycomb_grid) %&gt;%\n  # add grid ID\n  mutate(grid_id = 1:length(lengths(area_honeycomb_grid)))\n\nFollowing this, we can use lengths() and st_intersect() to determine the allocation of bus stop in each cell. The goal is to create a new column, consisting of the number of bus stop in each of the cell. The filter() function can then be added to remove all cells with no bus stop and create the final sf data frame.\n\n# Counting the number of bus stop in each cell\nhoneycomb_grid_sf$n_busstop = lengths(st_intersects(honeycomb_grid_sf,busstop))\n\n# Removing all cells without bus stop\nhoneycomb_count = filter(honeycomb_grid_sf, n_busstop &gt; 0)\n\nAt this point, the hexagon grid of bus stop can be drawn onto a map of Singapore using the functions of the tmap package. Additionally, the n_busstop column can be passed to the tm_fill() function to shade the cell based on the number of bus stops in it.\n\n\n\n\n\n\nNote\n\n\n\n\ntm_basemap: Choosing the basemap layer on which the hexagon grid will be drawn. OpenStreetMap is chosen due to its high fidelity while not being overly crowded. Additionally, OpenStreetMap displays icon for bus stops in Singapore, allowing user to visually check any cell.\n\nIf an incorrect CRS was specified in the earlier steps, the basemap will be of an incorrect location or alignment.\n\n\n\n\n\ntmap_mode('plot')\n\nbushexmap &lt;- tm_shape(honeycomb_count)+\n  tm_fill(\n    col = \"n_busstop\",\n    palette = \"Blues\",\n    style = \"cont\",\n    title = \"Number of bus stop\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of bus stop: \" = \"n_busstop\"\n    ),\n    popup.format = list(\n      n_busstop = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(main.title = 'Distribution of Bus Stops', main.title.position = 'center')\n\nbushexmap\n\n\n\n\nFrom the illustration, we can see that each cell might contain up to ten bus stops. A bar chat can be drawn with the ggplot2 package to visualize the distribution of number of bus stop in each cell.\n\nggplot(honeycomb_count, aes(x=n_busstop))+\n  geom_bar()+\n  theme_classic()+\n  geom_text(aes(label = ..count..), stat = \"count\", vjust = -0.5, colour = \"black\")\n\n\n\n\nAs can be seen, the majority of cells contain 1-2 bus stop with only 214 cells containing more than 6 bus stops. This shows that the cells adequately capture solitary bus stop, as well as pairs of bus stops (bus stops which are opposite each other, served by the same bus services).\n\n\nCombining Aspatial and Geospatial Data\nIn order to conduct geospatial analysis, a data frame which contains the hexagon cells as well as the number of bus trips for each cells must be created. This can be done using the left_join argument.\n\n\n\n\n\n\nNote\n\n\n\nThere are important arguments which can be used to create a cleaner combined data frame.\n\nby = join_by(BUS_STOP_N == ORIGIN_PT_CODE)): Indicate the column by which the two data frames can be matched and joined. In this case, the bus stop code will be used.\nselect(1,4,5): Indicate the index number of the columns to be kept in the final data frame. Only the bus stop number (column 1), total number of trips (column 4), and geometry (column 5) will be kept.\nreplace(is.na(.),0): Replace all value of NA with 0. This is to ensure that bus stop with no trips in a given time frame is accurately tallied at 0.\n\n\n\n\n# Weekday morning peak 6am - 9am\npassenger_wd_69_combined &lt;- left_join(busstop, passenger_wd_69_tallied, by = join_by(BUS_STOP_N == ORIGIN_PT_CODE))%&gt;%\n  select(1,4,5)%&gt;%\n  replace(is.na(.),0)\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\npassenger_wd_1720_combined &lt;- left_join(busstop, passenger_wd_1720_tallied, by = join_by(BUS_STOP_N == ORIGIN_PT_CODE))%&gt;%\n  select(1,4,5)%&gt;%\n  replace(is.na(.),0)\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\npassenger_weh_1114_combined &lt;- left_join(busstop, passenger_weh_1114_tallied, by = join_by(BUS_STOP_N == ORIGIN_PT_CODE))%&gt;%\n  select(1,4,5)%&gt;%\n  replace(is.na(.),0)\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\npassenger_weh_1619_combined &lt;- left_join(busstop, passenger_weh_1619_tallied, by = join_by(BUS_STOP_N == ORIGIN_PT_CODE))%&gt;%\n  select(1,4,5)%&gt;%\n  replace(is.na(.),0)\n\nIt is important to note that the bus stops and their total trips have not been tallied into the hexagon cells. st_join() can be used to accomplish this for each time frame.\n\n\n\n\n\n\nNote\n\n\n\nThe by argument in st_join() can be passed the function st_within to specify that we would like to join the two data frames where the geometry in the latter is within the geometry of the former. In this case, it would mean that two rows will be joined where the bus stop lies within a particular polygon.\n\nThe group_by() and summarise() functions here are used similarly to before, they sums up the total number of trips for all the bus stops in the hexagon, based on its grid_id, and create a new column called TOTAL_TRIP.\n\n\n\n\n# Weekday morning peak 6am - 9am\nhex_passenger_wd_69 &lt;- st_join(honeycomb_count, passenger_wd_69_combined, by = st_within(sparse = FALSE), largest = TRUE) %&gt;%\n  group_by(grid_id)%&gt;%\n  summarise(TOTAL_TRIP = sum(TRIPS))\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nhex_passenger_wd_1720 &lt;- st_join(honeycomb_count, passenger_wd_1720_combined, by = st_within(sparse = FALSE), largest = TRUE) %&gt;%\n  group_by(grid_id)%&gt;%\n  summarise(TOTAL_TRIP = sum(TRIPS))\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nhex_passenger_weh_1114 &lt;- st_join(honeycomb_count, passenger_weh_1114_combined, by = st_within(sparse = FALSE), largest = TRUE) %&gt;%\n  group_by(grid_id)%&gt;%\n  summarise(TOTAL_TRIP = sum(TRIPS))\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nhex_passenger_weh_1619 &lt;- st_join(honeycomb_count, passenger_weh_1619_combined, by = st_within(sparse = FALSE), largest = TRUE) %&gt;%\n  group_by(grid_id)%&gt;%\n  summarise(TOTAL_TRIP = sum(TRIPS))\n\nIn sum, the analysis will revolve around the four following data frames which contain the spatial information of the hexagon as well as the total number of trips for each interested time frame:\n\nhex_passenger_wd_69: Weekday morning peak 6am - 9am\nhex_passenger_wd_1720: Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nhex_passenger_weh_1114: Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nhex_passenger_weh_1619: Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)"
  },
  {
    "objectID": "Personal_Proj/LISA/Take-Home_Ex1.html#cleanup-step",
    "href": "Personal_Proj/LISA/Take-Home_Ex1.html#cleanup-step",
    "title": "Analysis of Geospatial Distribution of Bus Ridership by Origin Bus Stop in Singapore",
    "section": "Cleanup Step",
    "text": "Cleanup Step\nBefore we move on to Geovisualization and the analysis of LISA, it would be wise to remove objects which will no longer be used. This will help to free up memory for other tasks. rm() can be used to perform this task\n\nrm(list = c('area_honeycomb_grid', 'bushexmap', 'honeycomb_count', 'honeycomb_grid_sf',       'passenger_wd_1720', 'passenger_wd_1720_tallied', 'passenger_wd_1720_combined',    'passenger_wd_69', 'passenger_wd_69_tallied', 'passenger_wd_69_combined',      'passenger_weh_1114', 'passenger_weh_1114_tallied', 'passenger_weh_1114_combined',      'passenger_weh_1619', 'passenger_weh_1619_tallied', 'passenger_weh_1619_combined'))"
  },
  {
    "objectID": "Personal_Proj/LISA/Take-Home_Ex1.html#exploratory-data-analysis",
    "href": "Personal_Proj/LISA/Take-Home_Ex1.html#exploratory-data-analysis",
    "title": "Analysis of Geospatial Distribution of Bus Ridership by Origin Bus Stop in Singapore",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nDescriptive Analysis of Bus Trips\nThe beginning step of the analysis would be to visualize the distribution of bus trips on the hexagon layer. This can be accomplished with the mapping functions of the tmap package. However, unlike the geovisualization of bus stop per hexagon, the number of trips for each hexagon depending on the time frame varies widely. Let’s confirm this by drawing a histogram of the distribution of trips in each time frame.\n\nweekday_morning_hist &lt;- ggplot(hex_passenger_wd_69, aes(x=TOTAL_TRIP))+\n  geom_histogram()+\n  scale_x_continuous(labels = scales::comma)+\n  ggtitle('Weekday Morning')+\n  theme_classic()\n\nweekday_afternoon_hist &lt;- ggplot(hex_passenger_wd_1720, aes(x=TOTAL_TRIP))+\n  geom_histogram()+\n  scale_x_continuous(labels = scales::comma)+\n  ggtitle('Weekday Evening')+\n  theme_classic()\n  \nweekend_morning_hist &lt;- ggplot(hex_passenger_weh_1114, aes(x=TOTAL_TRIP))+\n  geom_histogram()+\n  scale_x_continuous(labels = scales::comma)+\n  ggtitle('Weekend Morning')+\n  theme_classic()\n  \nweekend_evening_hist &lt;- ggplot(hex_passenger_weh_1619, aes(x=TOTAL_TRIP))+\n  geom_histogram()+\n  scale_x_continuous(labels = scales::comma)+\n  ggtitle('Weekend Evening')+\n  theme_classic()\n  \n  \ngridExtra::grid.arrange(weekday_morning_hist, weekday_afternoon_hist, weekend_morning_hist, weekend_evening_hist, nrow = 2, ncol = 2)\n\n\n\n\nUpon a brief inspection, it is possible to see that the range of trips between the different time periods are very different from each other, with Weekday Evening trips going above 300,000 for some hexagons, while Weekend Noon only ranging around 80,000 for its hexagons. By plotting this on map, we will get to see the geospatial distribution of the number of trips for each time frame.\nBefore plotting, the summary() function can be used to compute the average number of trips for each time frame. By seeing the quantile statistics, the difference in trips between each time frame can be better illuminated.\n\n# Apply summary() to each data frame\nweekday_morning_summary &lt;- summary(hex_passenger_wd_69$TOTAL_TRIP)\nweekday_afternoon_summary &lt;- summary(hex_passenger_wd_1720$TOTAL_TRIP)\nweekend_morning_summary &lt;- summary(hex_passenger_weh_1114$TOTAL_TRIP)\nweekend_evening_summary &lt;- summary(hex_passenger_weh_1619$TOTAL_TRIP)\n\n# Combine the summary results into one data frame\nsummary_df &lt;- data.frame(unclass(weekday_morning_summary), unclass(weekday_afternoon_summary), unclass(weekend_morning_summary), unclass(weekend_evening_summary))\n\ncolnames(summary_df) &lt;- c('Weekday Morning', 'Weekday Afternoon', 'Weekend Morning', 'Weekend Evening')\n\nsummary_df\n\n        Weekday Morning Weekday Afternoon Weekend Morning Weekend Evening\nMin.              0.000             0.000           0.000           0.000\n1st Qu.         202.500           469.750         119.000         153.750\nMedian         1282.000          1526.000         560.500         564.000\nMean           3862.767          3874.821        1436.913        1469.692\n3rd Qu.        4405.250          3716.500        1652.250        1466.500\nMax.         136490.000        310285.000       81616.000      106402.000\n\n\nFrom the summary table, it is possible to see that the two Weekday data frames resemble each other more while the two Weekend data frames are more similar. In terms of the Mean number of across hexagon cells, Weekday Morning and Weekday Afternoon are relatively similar. However, the Max number of trips for Weekday Afternoon is larger than Weekday Morning’s by roughly 127.3%. Similarly, Weekend/Holiday Evening and Weekend/Holiday Morning have a similar Mean number of trips, but the Max number for Weekend/Holiday Evening is higher than that of Weekend/Holiday Morning by 30.4%. Overall, the number of trips in all quantile are roughly double during the Weekday than compared to the Weekends.\n\n\nGeovisualization and Analysis\nBefore we map all four time frames, however, it is important to consider that the ‘style’ argument of tm_fill() can take on many values (pretty, quantile, equal, etc.). In order to pick an appropriate ‘style’, we can pick one time frame and plot it using three different styles to determine the best way to depict the distribution.\n\ntmap_mode('view')\n\n### Weekday morning peak 6am - 9am\n# Quantile style\nweekday_morning_quantile &lt;- tm_shape(hex_passenger_wd_69) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"quantile\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Morning Peak (Quantile)', title.position = c('right', 'top'), scale = 0.7)\n\n# Jenks style\nweekday_morning_jenks &lt;- tm_shape(hex_passenger_wd_69) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Morning Peak (Jenks)', title.position = c('right', 'top'), scale = 0.7)\n\n# Equal style\nweekday_morning_equal &lt;- tm_shape(hex_passenger_wd_69) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"equal\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Morning Peak (Equal)', title.position = c('right', 'top'), scale = 0.7)\n\ntmap_arrange(weekday_morning_quantile, weekday_morning_jenks, weekday_morning_equal, nrow = 3, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen, the ‘quantile’ and ‘jenks’ style can better display the difference in distribution of trips between the different hexagons, allowing for more differentiated shades. However, the ‘quantile’ function suffers from its final grouping, containing values from roughly 5,539 to 136,490; this resulted int the oversaturation of the high value hexagons. On the other hand, the ‘jenks’ method “divides the features into classes whose boundaries are where there are relatively big differences in the data values” (Reference). Therefore, it is possible to move forward using the ‘jenks’ style for visualization, but by adjusting the breaks to provide more stratification in the hexagon colors for better visualization..\n\nIt is good to remove the 3 plots created to plot the different styles since they will not be used and will only take up memory.\n\nrm(list=c('weekday_morning_equal','weekday_morning_quantile','weekday_morning_jenks'))\n\n\nThe functions of the tmap package will be used to create the maps.\n\ntmap_mode('view')\n\n# Weekday morning peak 6am - 9am\nweekday_morning &lt;- tm_shape(hex_passenger_wd_69) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Morning Peak', title.position = c('right', 'top'))\n\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nweekday_afternoon &lt;- tm_shape(hex_passenger_wd_1720) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Afternoon Peak', title.position = c('right', 'top'))\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nweekend_noon &lt;- tm_shape(hex_passenger_weh_1114) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekend/holiday Morning Peak', title.position = c('right', 'top'))\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nweekend_evening &lt;- tm_shape(hex_passenger_weh_1619) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekend/holiday Evening Peak', title.position = c('right', 'top'))\n\ntmap_arrange(weekday_morning, weekday_afternoon, weekend_noon, weekend_evening, nrow = 2, ncol = 2, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSearch for the bus stop near your place and compare the numbers between the four maps by zooming in! Do you think it’s accurate?\n\n\nFrom the rough visualization, it’s clear that not all bus stops experience a similar level of traffic throughout different timing of the days. Additionally, based on the quantiles created by tmap, it seems that the ranges of passenger traffic are radically different between the four time windows. For example, certain grids during Weekday Morning Peak, a hexagon could reach 328,545 passenger trips, whereas the highest number during Weekend/holiday Morning Peak only reaches 112,330. It appears that Weekend Evening Peak 17:00 - 20:00 is the period with the highest level of activity.\nAn important observation seems to be that there darker shaded hexagons tend to be clustered, indicating a positive spatial autocorrelation. However, there are clearly hexagons which are much darker than its surrounding tiles (such as the one near Tampines), indicating negative spatial autocorrelation. By conducting LISA analysis, it will be possible to determine the level of spatial autocorrelation for each hexagon, visualize them, as well as to depict the relationship between a hexagon and its neighbors through a LISA cluster map."
  },
  {
    "objectID": "Personal_Proj/LISA/Take-Home_Ex1.html#defining-the-neighborhood",
    "href": "Personal_Proj/LISA/Take-Home_Ex1.html#defining-the-neighborhood",
    "title": "Analysis of Geospatial Distribution of Bus Ridership by Origin Bus Stop in Singapore",
    "section": "Defining the Neighborhood",
    "text": "Defining the Neighborhood\nBefore the LISA analysis can be conducted, it is important to define our neighborhood, or the neighbors of each polygon. This is based on the ideas that neighbors, or spatial objects which are related to other spatial objects based on sharing a common boundary or lying with a certain distance of one another, might affect each other.\nIn this case, we would like to identify the neighbors of each hexagon so that LISA analysis can determine if the number of trips in a hexagon in a time frame is correlated to the number of trips of the hexagons around it, either in the same direction or opposite direction.\nThe first step to determining the neighborhood is to choose a method by which neighbors are classified:\n\nContiguity-based Method: Based on the sharing of boundaries, either edges and/or points (Queen and Rook method).\nDistance-based Method: Based on the distances between the centroid (central point of each hexagon) of each polygon. This can either be set to a distance where each hexagon has at least one neighbor (Fixed Distance) or where each polygon has a certain number of neighbors (Adaptive Distance).\n\nIt is important to choose the appropriate method according to each situation. However, in this study, Contiguity-based methods can be preemptively ruled out due to the fact that some cells have no contiguous neighbors, as can be seen below.\n\n\n\nIsolated Cells\n\n\nIf the contiguity method is used, the LISA calculation for these cells would not be conducive for analysis as they technically have no neighbors. This can be confirmed by using the st_contiguity() function to create a Queen contiguity matrix for one time frame\n\nwm_q &lt;- st_contiguity(st_geometry(hex_passenger_wd_1720))\n\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 1520 \nNumber of nonzero links: 6874 \nPercentage nonzero weights: 0.2975242 \nAverage number of links: 4.522368 \n9 regions with no links:\n561 726 980 1047 1415 1505 1508 1512 1520\n19 disjoint connected subgraphs\nLink number distribution:\n\n  0   1   2   3   4   5   6 \n  9  39 109 205 291 364 503 \n39 least connected regions:\n1 7 22 38 98 169 187 195 211 218 258 259 264 267 287 454 562 607 642 696 708 732 751 784 869 1021 1022 1046 1086 1214 1464 1471 1482 1500 1501 1503 1506 1510 1519 with 1 link\n503 most connected regions:\n10 13 16 17 24 25 31 35 42 43 48 53 55 60 63 67 73 77 80 81 84 85 87 88 91 92 97 102 107 111 117 121 127 133 140 141 143 148 149 150 154 155 156 157 163 164 165 173 174 175 183 184 185 191 192 193 194 200 201 202 205 206 207 208 216 229 239 243 244 246 257 266 271 278 279 283 284 291 292 298 300 301 302 304 309 310 312 313 316 321 324 325 327 337 338 339 340 343 352 355 363 368 390 391 400 402 403 407 414 418 423 425 431 436 437 438 440 443 450 451 452 461 466 467 468 469 473 477 480 481 485 489 493 494 496 502 503 507 513 514 517 518 523 529 534 539 543 548 549 550 552 556 558 564 568 573 574 576 577 581 590 591 594 598 599 604 605 609 615 619 624 626 633 636 637 638 648 649 650 654 655 657 658 659 669 670 671 677 680 681 682 687 688 690 691 700 701 704 705 706 713 716 717 724 727 728 729 740 741 755 757 758 760 771 774 775 776 777 782 783 787 788 789 792 793 794 795 799 800 806 807 810 811 812 813 819 820 823 824 825 830 831 832 841 843 844 846 847 848 850 851 852 853 854 860 863 865 866 867 871 872 876 877 878 880 881 882 884 885 887 888 891 893 896 899 902 905 906 910 914 919 921 926 927 928 930 931 935 937 943 944 945 946 947 948 954 958 959 962 963 968 969 971 972 973 977 984 985 986 987 988 990 996 997 998 999 1004 1011 1012 1013 1014 1024 1025 1026 1028 1029 1036 1037 1038 1042 1050 1051 1054 1056 1057 1062 1063 1064 1066 1067 1068 1069 1076 1078 1079 1080 1083 1089 1093 1100 1101 1102 1105 1106 1110 1111 1117 1120 1121 1122 1128 1133 1134 1135 1136 1141 1142 1144 1145 1146 1147 1148 1150 1156 1157 1158 1162 1163 1164 1166 1169 1170 1171 1172 1176 1177 1178 1179 1180 1184 1186 1190 1191 1192 1193 1194 1201 1202 1203 1204 1205 1206 1207 1210 1211 1217 1218 1219 1220 1221 1227 1233 1234 1235 1239 1244 1245 1251 1253 1254 1255 1261 1265 1266 1271 1272 1273 1277 1281 1283 1289 1299 1301 1302 1303 1304 1316 1318 1324 1325 1326 1327 1329 1330 1331 1334 1335 1336 1337 1343 1344 1345 1352 1353 1355 1356 1361 1365 1366 1368 1369 1371 1372 1377 1380 1381 1382 1384 1388 1391 1393 1395 1398 1406 1408 1412 1417 1418 1420 1424 1425 1426 1427 1428 1433 1434 1435 1436 1440 1441 1442 1446 1447 1449 1451 1453 1456 1457 1459 1460 1461 1462 1469 with 6 links\n\n\nAs can be seen from the weight matrix, 10 hexagon cells have 0 neighbor. Therefore, a Distance-based Method would be suitable for analysis. Both Fixed Distance and Adaptive Distance Weight Matrix can be created for comparison to find the most appropriate method.\n\nFinding the Appropriate Distance Matrix\n\nCreating the Fixed Distance Matrix\nst_dist_band() of sfdep is incredibly powerful in that it can create a neighbor list based on distance between the centroid of polygons and a lower and upper bound distance to other centroid. The default arguments for st_dist_band() will define a lower and upper bound distance from the centroid of a polygon so that each hexagon will have at least one neighbor. This is the equivalent of the steps in spdep of the function knearneigh() of k=1.\nst_inverse_distance() of a sfdep can be combined with st_dist_band() in a dplyr step to create a new column in each data frame of the different time frames to create a neighbor list and a inverse distance weight list. Additionally, st_lag() can be used to create a spatially lagged value column for total trips based on the weight of neighbors.\n\n# Weekday morning peak 6am - 9am\nwd69_nb &lt;- hex_passenger_wd_69 %&gt;%\n  mutate(nb = st_dist_band(area_honeycomb_grid),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nwd1720_nb &lt;- hex_passenger_wd_1720 %&gt;%\n  mutate(nb = st_dist_band(area_honeycomb_grid),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nweh1114_nb &lt;- hex_passenger_weh_1114 %&gt;%\n  mutate(nb = st_dist_band(area_honeycomb_grid),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nweh1619_nb &lt;- hex_passenger_weh_1619 %&gt;%\n  mutate(nb = st_dist_band(area_honeycomb_grid),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\n\n\nExamining the Fixed Distance Method\nSince the hexagon tiles is stable across all time frame, it is possible to plot the neighbor relationship for only one time frame in order to visualize the neighbors list created. Let’s take Weekday morning peak 6am - 9am. By visualizing, the appropriateness of the Fixed Distance Method can be roughly determined.\n\nplot(wd69_nb$area_honeycomb_grid, border = 'lightgrey')\nplot(wd69_nb$nb, st_centroid(wd69_nb$area_honeycomb_grid), add=TRUE)\nplot(st_knn(wd69_nb$area_honeycomb_grid, k = 1), st_centroid(wd69_nb$area_honeycomb_grid), add=TRUE, col = 'red', length = 0.08)\n\n\n\n\nDue to the large number of hexagons, the visualization is too dense to delineate any helpful details. However, the summary() function might be of some help to study the neighbors list. Similar to be fore, we ony need to test one time frame.\n\nsummary(wd69_nb$nb)\n\nNeighbour list object:\nNumber of regions: 1520 \nNumber of nonzero links: 76822 \nPercentage nonzero weights: 3.325052 \nAverage number of links: 50.54079 \n2 disjoint connected subgraphs\nLink number distribution:\n\n 1  2  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 \n 2  1  5  1  5  5  1  4  3  3  4  4  3 12 10  8  7  7  9 11 14  7 10 12 13 15 \n31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 \n27 21 15 15 19 18 19 26 25 28 29 33 22 28 24 32 32 25 26 36 32 33 43 35 31 45 \n57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 \n31 35 35 36 48 35 36 47 39 47 38 37 27 32 30 16 25 13 13  3  2 \n2 least connected regions:\n1505 1520 with 1 link\n2 most connected regions:\n729 1076 with 77 links\n\n\nAs can be seen, there are many hexagons with less than 30 neighbors. However, the average number of links is still over 50. This is a draw back of the Fixed Distance method, hexagon cells in denser areas will have more neighbors while those in the periphery or isolated positions will have fewer neighbors. Yet, this lack of neighbor might affect the LISA calculation as the effect of spatial autocorrelation might be smoothed out for those cells with many neighbors\nBetween the Fixed Distance and Adaptive Distance Matrix, an Adaptive Distance Matrix would be more appropriate to the non-uniform nature of bus stop spatial distribution across the map. Using Adaptive Distance would allow for the specification of the number of neighbors, allowing for the standardisation of the LISA analysis process across hexagons.\n\n\n\nCreating the Adaptive Distance Matrix\nThe creation of the Adaptive Distance Weight list is largely similar to that of the Fixed Distance Weight list thanks to the sfdep package. The only major amendment is the usage of st_knn() instead of st_distance_band(). st_knn() allows for the forcing of a certain of neighbors for each hexagon cell. This can be accomplished by passing the k argument to st_knn().\n\n# Weekday morning peak 6am - 9am\nwd69_nb &lt;- hex_passenger_wd_69 %&gt;%\n  mutate(nb = st_knn(area_honeycomb_grid, k = 6),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\n\n\nwd1720_nb &lt;- hex_passenger_wd_1720 %&gt;%\n  mutate(nb = st_knn(area_honeycomb_grid, k = 6),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nweh1114_nb &lt;- hex_passenger_weh_1114 %&gt;%\n  mutate(nb = st_knn(area_honeycomb_grid, k = 6),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nweh1619_nb &lt;- hex_passenger_weh_1619 %&gt;%\n  mutate(nb = st_knn(area_honeycomb_grid, k = 6),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\nSimilar to before, the summary() function can be used to check the average number of neighbors for each cell.\n\nsummary(wd69_nb$nb)\n\nNeighbour list object:\nNumber of regions: 1520 \nNumber of nonzero links: 9120 \nPercentage nonzero weights: 0.3947368 \nAverage number of links: 6 \nNon-symmetric neighbours list\nLink number distribution:\n\n   6 \n1520 \n1520 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 with 6 links\n1520 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 with 6 links\n\n\nIt can be seen that all hexagon cells now have 6 neighbors. The LISA calculation can commence."
  },
  {
    "objectID": "Personal_Proj/LISA/Take-Home_Ex1.html#local-indicators-of-spatial-autocorrelation-lisa",
    "href": "Personal_Proj/LISA/Take-Home_Ex1.html#local-indicators-of-spatial-autocorrelation-lisa",
    "title": "Analysis of Geospatial Distribution of Bus Ridership by Origin Bus Stop in Singapore",
    "section": "Local Indicators of Spatial Autocorrelation (LISA)",
    "text": "Local Indicators of Spatial Autocorrelation (LISA)\nThe statistical test used for LISA in this study will be the Local Moran’s I. The hypothesis for the Local Moran’s I will be as follow for each time frame:\n\nNull Hypothesis (H0): No Spatial Autocorrelation\nAlternative Hypothesis (H1): There is Spatial Autocorrelation, Positive or Negative\n\nThe result of the test can be determined using the z-score or p-value, we will be using the p-value.\n\nCalculating Local Moran’s I\nlocal_moran() of the sfdep package can be used to calculate Local Moran’s I Statistic and other related statistics. the unnest() function is used to unpack the Local Moran’s statistics into separate columns.\nNote some important arguments when using the local_moran() function.\n\n\n\n\n\n\nNote\n\n\n\n\nnsim: The number of simulations to run. The simulations will create random patterns on the map by reassigning the values and calculate the p-value as the proportions of values as extreme or more extreme than the actual observed values.\nalternative = ‘two.sided’: The default test for local_moran() is a two-sided tests, which is aligned with the H1. ‘greater’ or ‘less’ can be passed to alternative to conduct other types of tests.\n\n\n\n\n# Weekday morning peak 6am - 9am\nwd69_lisa &lt;- wd69_nb %&gt;%\n  mutate(local_moran = local_moran(TOTAL_TRIP, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nwd1720_lisa &lt;- wd1720_nb %&gt;%\n  mutate(local_moran = local_moran(TOTAL_TRIP, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nweh1114_lisa &lt;- weh1114_nb %&gt;%\n  mutate(local_moran = local_moran(TOTAL_TRIP, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nweh1619_lisa &lt;- weh1619_nb %&gt;%\n  mutate(local_moran = local_moran(TOTAL_TRIP, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Examining the first 5 rows of the new data frame\nhead(weh1619_lisa)\n\nSimple feature collection with 6 features and 17 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 3720.122 ymin: 27925.48 xmax: 4970.122 ymax: 31533.92\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 18\n      ii      eii  var_ii  z_ii  p_ii p_ii_sim p_folded_sim skewness kurtosis\n   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 0.0525  0.00186 0.0160  0.401 0.689     0.05        0.025   -10.5    126.  \n2 0.0695 -0.00296 0.0171  0.554 0.580     0.1         0.05     -5.71    39.0 \n3 0.0893  0.0180  0.00784 0.806 0.421     0.09        0.045    -5.10    38.8 \n4 0.0585  0.00966 0.00233 1.01  0.312     0.03        0.015    -2.74     9.94\n5 0.0719  0.00758 0.00521 0.891 0.373     0.01        0.005    -4.65    34.0 \n6 0.0975  0.00466 0.0137  0.794 0.427     0.03        0.015    -4.54    30.6 \n# ℹ 9 more variables: mean &lt;fct&gt;, median &lt;fct&gt;, pysal &lt;fct&gt;, nb &lt;nb&gt;,\n#   wt &lt;list&gt;, lag_trip &lt;dbl&gt;, grid_id &lt;int&gt;, TOTAL_TRIP &lt;dbl&gt;,\n#   area_honeycomb_grid &lt;POLYGON [m]&gt;\n\n\nA host of statistics related to the Local Moran’s I have been added to the data frame including the I statistic (ii), the p-value (p_ii), and the mean cluster (mean). This will help us to create visualizations that will shed light onto the phenomenon of spatial autocorrelation in the different time frames.\n\n\nMapping Local Moran’s I\n\nMapping Local Moran’s I Values and p-values\nIn order to effectively visualize the Local Moran’s I statistics for each time frame, it is preferable to plot the Local Moran’s I values and p-values together. At this point, creating the visualization for each time frame separately instead of all together will allow for easier analysis.\n\n\n\n\n\n\nNote\n\n\n\nAs we would like to only display p-values which indicate statistical significance, we will use a filter and create a custom color palette to indicate that any p-value above 0.05 will not be displayed.\n\n\n\n\n\n\n\n\nNote\n\n\n\nSeveral functions are added to make the map interactive and aesthetically pleasing\n\ntmap_mode(‘view’): Creates an interactive map which allow zooming and interacting with cells on the map\npop.vars: Identifying the legend and value which pops up when a cell is selected. In this case, it is the number of bus stops.\npopup.format: Specifying the format of the variable to be displayed when selecting a cell.\n\n\n\nWeekday Morning Peak 6am - 9am\n\n\nShow the code\np_value_color = c('#bdd7e7','#6baed6', '#2171b5')\n\ntmap_mode('view')\n\n# Spatially Lagged Values\nwd69.lag &lt;- tm_shape(wd69_lisa)+\n  tm_fill('lag_trip',\n          style = \"jenks\", \n          title = \"Spatially Lagged Trips\",\n          id = 'grid_id',\n          popup.vars = c(\n            \"Spatially Lagged Trips: \" = \"lag_trip\"),\n          popup.format = list(\n            lag_trip = list(format = \"f\", digits = 0)))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Weekend/holiday Morning Spatially Lagged Trips\", main.title.position = \"center\")\n\n# Local Moran's I Statistics\nwd69.localmi &lt;- wd69_lisa %&gt;%\n  filter(p_ii &lt; 0.05) %&gt;%\n  tm_shape()+\n  tm_fill('ii',\n          style = \"pretty\", \n          title = \"Local Moran's I Statistics\",\n          id = 'grid_id',\n          popup.vars = c(\n            \"I Statistic: \" = \"ii\"),\n          popup.format = list(\n            ii = list(format = \"f\", digits = 5)))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Weekday Morning Local Moran's I values\", main.title.position = \"center\")\n\n# Local Moran's I p-values\nwd69.pvalue &lt;- wd69_lisa %&gt;%\n  filter(p_ii &lt; 0.05) %&gt;%\n  tm_shape()+\n  tm_fill('p_ii',\n          breaks = c(-Inf, 0.001, 0.01, 0.05),\n          palette = rev(p_value_color),\n          title = \"Local Moran's I p-values\",\n          id = 'grid_id',\n          popup.vars = c(\n            \"p-value: \" = \"p_ii\"),\n          popup.format = list(\n            p_ii = list(format = \"f\", digits = 5)))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Weekday Morning Local Moran's I p-values\", main.title.position = \"center\")\n\ntmap_arrange(wd69.lag, wd69.localmi, wd69.pvalue, nrow = 3, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the illustrations, it can be determined that there is spatial clustering in the number of trips, especially in the pattern of hexagons with high number of spatially lagged trips surrounding one with a fewer spatially lagged trips; this pattern is notable around the Jurong and Choa Chu Kang areas in the West and Tampines Area in the East. One can also note this pattern in the Ang Mo Kio and Yishun area in the North and those hexagons around the Woodlands causeway. This is reinforced when one looks at the p-values plot. The surrounding polygons with high spatially lagged trips often have a p-value of less than 0.001, which indicates statistically significant spatial autocorrelation. Their Local Moran’s I Statistics are relatively close to zero, either in the positive or negative direction. They have a small, negative Local Moran’s I, suggesting that they are surrounded by neighbours which are dissimilar to them, likely due to their high spatially lagged values. A possible interpretation of this result is that these are major population centres, also known as the Heartlands, where people in the surrounding areas are congregating due to them hosting major bus interchanges in the morning to get to places of employment or education.\nWeekday Afternoon Peak 5pm - 8pm (17:00 - 20:00)\n\n\nShow the code\ntmap_mode('view')\n\n# Spatially Lagged Values\nwd1720.lag &lt;- tm_shape(wd1720_lisa)+\n  tm_fill('lag_trip',\n          style = \"jenks\", \n          title = \"Spatially Lagged Trips\",\n          id = 'grid_id',\n          popup.vars = c(\n            \"Spatially Lagged Trips: \" = \"lag_trip\"),\n          popup.format = list(\n            lag_trip = list(format = \"f\", digits = 0)))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Weekend/holiday Morning Spatially Lagged Trips\", main.title.position = \"center\")\n\n# Local Moran's I Statistics\nwd1720.localmi &lt;- wd1720_lisa %&gt;%\n  filter(p_ii &lt; 0.05) %&gt;%\n  tm_shape()+\n  tm_fill('ii',\n          style = \"pretty\", \n          title = \"Local Moran's I Statistics\",\n          id = 'grid_id',\n          popup.vars = c(\n            \"I Statistic: \" = \"ii\"),\n          popup.format = list(\n            ii = list(format = \"f\", digits = 5)))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Weekday Afternoon Local Moran's I values\", main.title.position = \"center\")\n\n# Local Moran's I p-values\nwd1720.pvalue &lt;- wd1720_lisa %&gt;%\n  filter(p_ii &lt; 0.05) %&gt;%\n  tm_shape()+\n  tm_fill('p_ii',\n          breaks = c(-Inf, 0.001, 0.01, 0.05),\n          palette = rev(p_value_color),\n          title = \"Local Moran's I p-values\",\n          id = 'grid_id',\n          popup.vars = c(\n            \"p-value: \" = \"p_ii\"),\n          popup.format = list(\n            p_ii = list(format = \"f\", digits = 5)))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Weekday Afternoon Local Moran's I p-values\", main.title.position = \"center\")\n\ntmap_arrange(wd1720.lag, wd1720.localmi, wd1720.pvalue, nrow = 3, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Local Moran’s I result for the Weekday afternoon peak 5pm - 8pm is similar to that of the weekday morning peak. A similar pattern of hexagons with high number of spatially lagged trips and statistically significant p-value surrounding those with fewer spatially lagged trips and non-significant p-value reappear here. Interestingly, one of these clusters in Jurong in the West and the two in Ang Mo Kio and Yishun in the North seem to have dissipated in this time window. However, the cluster in Tampines, Choa Chu Kang, and the Woodlands causeway remain. This reinforces the previous observation that these areas consist of major interchanges where the flows of people would congregate before dispersing again. Possibly, this is due to the flow of people returning home after work. This is reinforced by the emergence of individual cells near the Marina Reservoir, also known as part of the Downtown Core, and the office buildings near Harbourfront in the South. They have statistically significant p-value and relatively high spatially lagged trips, possibly indicating flow of people leaving their workplaces.\nWeekend/holiday Morning Peak 11am - 2pm (11:00 - 14:00)\n\n\nShow the code\ntmap_mode('view')\n\n# Spatially Lagged Values\nweh1114.lag &lt;- tm_shape(weh1114_lisa)+\n  tm_fill('lag_trip',\n          style = \"jenks\", \n          title = \"Spatially Lagged Trips\",\n          id = 'grid_id',\n          popup.vars = c(\n            \"Spatially Lagged Trips: \" = \"lag_trip\"),\n          popup.format = list(\n            lag_trip = list(format = \"f\", digits = 0)))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Weekend/holiday Morning Spatially Lagged Trips\", main.title.position = \"center\")\n\n# Local Moran's I Statistics\nweh1114.localmi &lt;- weh1114_lisa %&gt;%\n  filter(p_ii &lt; 0.05) %&gt;%\n  tm_shape()+\n  tm_fill('ii',\n          style = \"pretty\", \n          title = \"Local Moran's I Statistics\",\n          id = 'grid_id',\n          popup.vars = c(\n            \"I Statistic: \" = \"ii\"),\n          popup.format = list(\n            ii = list(format = \"f\", digits = 5)))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Weekend/holiday Morning Local Moran's I values\", main.title.position = \"center\")\n\n# Local Moran's I p-values\nweh1114.pvalue &lt;- weh1114_lisa %&gt;%\n  filter(p_ii &lt; 0.05) %&gt;%\n  tm_shape()+\n  tm_fill('p_ii',\n          breaks = c(-Inf, 0.001, 0.01, 0.05),\n          palette = rev(p_value_color),\n          title = \"Local Moran's I p-values\",\n          id = 'grid_id',\n          popup.vars = c(\n            \"p-value: \" = \"p_ii\"),\n          popup.format = list(\n            p_ii = list(format = \"f\", digits = 5)))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Weekend/holiday Morning Local Moran's I p-values\", main.title.position = \"center\")\n\ntmap_arrange(weh1114.lag, weh1114.localmi, weh1114.pvalue, nrow = 3, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn addition to the clusters in the Heartlands as seen previously, the Weekend/holiday Noon Peak 11am – 2pm (11:00 – 14:00) result shows a pattern of cells with statistically significant p-value emerge in the central of Singapore, in the areas of Tiong Bahru, Outram Park, and Orchard Road. These cells are surrounded by other cells with high spatially lagged trips but non-significant p-values. A possible interpretation for these cells is that these are popular areas for people to congregate on the weekends, increasing their number of trips, and due to their proximity, they exhibit statistically significant spatial autocorrelation. A more micro-level analysis might reveal why the bus stops within these areas are more prominent than others. For Orchard, road, the interpretation might be more straightforward, as perennially popular location for locals and tourists alike.\nWeekend/holiday Evening Peak 4pm - 7pm (16:00 - 19:00)\n\n\nShow the code\ntmap_mode('view')\n\n# Spatially Lagged Values\nweh1619.lag &lt;- tm_shape(weh1619_lisa)+\n  tm_fill('lag_trip',\n          style = \"jenks\", \n          title = \"Spatially Lagged Trips\",\n          id = 'grid_id',\n          popup.vars = c(\n            \"Spatially Lagged Trips: \" = \"lag_trip\"),\n          popup.format = list(\n            lag_trip = list(format = \"f\", digits = 0)))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Weekend/holiday Morning Spatially Lagged Trips\", main.title.position = \"center\")\n\n# Local Moran's I Statistics\nweh1619.localmi &lt;- weh1619_lisa %&gt;%\n  filter(p_ii &lt; 0.05) %&gt;%\n  tm_shape()+\n  tm_fill('ii',\n          style = \"pretty\", \n          title = \"Local Moran's I Statistics\",\n          id = 'grid_id',\n          popup.vars = c(\n            \"I Statistic: \" = \"ii\"),\n          popup.format = list(\n            ii = list(format = \"f\", digits = 5)))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Weekend/holiday Morning Local Moran's I values\", main.title.position = \"center\")\n\n# Local Moran's I p-values\nweh1619.pvalue &lt;- weh1619_lisa %&gt;%\n  filter(p_ii &lt; 0.05) %&gt;%\n  tm_shape()+\n  tm_fill('p_ii',\n          breaks = c(-Inf, 0.001, 0.01, 0.05),\n          palette = rev(p_value_color),\n          title = \"Local Moran's I p-values\",\n          id = 'grid_id',\n          popup.vars = c(\n            \"p-value: \" = \"p_ii\"),\n          popup.format = list(\n            p_ii = list(format = \"f\", digits = 5)))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Weekend/holiday Morning Local Moran's I p-values\", main.title.position = \"center\")\n\ntmap_arrange(weh1619.lag, weh1619.localmi, weh1619.pvalue, nrow = 3, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLastly, for the Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00) Local Moran’s I result, it is possible to find what seems to be the converge of other the patterns in the previous time windows. There are clusters of statistically significant cells with higher spatially lagged trips emerging in the Heartlands, Tiong Bahru, Orchard Road, and Harbourfront; the notable exception would be the lack of statistically significant cells in the Downtown Core. Aside from the pattern in major population centres, the pattern reinforces the previous observation that there is more activity at bus stops in popular areas for locals and tourists to visit and return during the evening peak period. A possible case for this would be the single cell on Sentosa island, a mostly purely leisure destination.\nOverall, the geospatial autocorrelation pattern that seems to emerge is the movement of people to and from work during the weekday and to leisure spots during the weekend and holiday. This pattern reveals itself through hexagons with higher level of spatially lagged trips, often bus interchanges in population centre, being surrounded by less similar neighbors. Or, hexagons with higher level of spatially lagged trips, often in leisure spots, being nearer to more similar neighbors and hexagons with higher spatially lagged trips but non-significant autocorrelation in the Downtown Core.\nThe observation made can further be examined by visualizing the structure of the relationship between the values of a hexagon and its similarity to its neighbors, or what is categorized in Local Moran’s I as low-low, low-high, high-low, high-high. This is can be done using the Moran Scatterplot andn LISA Cluster Map.\n\n\nMoran Scatterplot\nThe Moran Scatterplot assign each of the hexagon cell of bus stops to one of four quadrants:\n\nHigh - High: Areas of high values surrounded by similar neighbors\nLow - Low: Areas of low values surrounded by similar neighbors\nHigh - Low: Areas of high values surrounded by dissimilar neighbors\nLow - High: Areas of low values surrounded by dissimilar neighbors\n\nThe Moran Scatterplot visualization allows users to roughly comprehend the spatial clustering in the data set. This can be done using the moran.plot() function of the spdep package.\n\n\n\n\n\n\nNote\n\n\n\nDue to the usage of sfdep package, certain functions must be combined with the lisa objects in order for moran.plot() to work correctly.\n\nscale(): The scale function works to center and scale the values. This involves subtracting the value by the mean then dividing it by the standard deviation.\nas.vector(): This function transforms the scaled values into a more easily plottable object.\nnb2listw(): This function transform the neighbor list into a listw object which can be accepted by moran.plot()\n\n\n\n\npar(mfrow = c(2,2))\n\n# Weekday morning peak 6am - 9am\nwd69_scatter &lt;- moran.plot(as.vector(scale(wd69_lisa$TOTAL_TRIP)), nb2listw(wd69_lisa$nb),\n           labels = as.character(wd69_lisa$grid_id),\n           xlab = 'z-Trip',\n           ylab = 'Spatially Lagged z-Trip',\n           main = 'Weekday Morning')\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nwd1720_scatter &lt;- moran.plot(as.vector(scale(wd1720_lisa$TOTAL_TRIP)), nb2listw(wd1720_lisa$nb),\n           labels = as.character(wd1720_lisa$grid_id),\n           xlab = 'z-Trip',\n           ylab = 'Spatially Lagged z-Trip',\n           main = 'Weekday Afternoon')\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nweh1114_scatter &lt;- moran.plot(as.vector(scale(weh1114_lisa$TOTAL_TRIP)), nb2listw(weh1114_lisa$nb),\n           labels = as.character(weh1114_lisa$grid_id),\n           xlab = 'z-Trip',\n           ylab = 'Spatially Lagged z-Trip',\n           main = 'Weekend/holiday Noon')\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nweh1619_scatter &lt;- moran.plot(as.vector(scale(weh1619_lisa$TOTAL_TRIP)), nb2listw(weh1619_lisa$nb),\n           labels = as.character(weh1619_lisa$grid_id),\n           xlab = 'z-Trip',\n           ylab = 'Spatially Lagged z-Trip',\n           main = 'Weekend/holiday Evening')\n\n\n\n\nThe draw back of the Moran Scatterplot is that we cannot visualize the spatial distribution of the hexagons, as well as the fact that it does not indicate significance of the Local Moran’s I. Additionally, due to density of certain segments, it is difficult to discern their position.\n\n\nLISA Cluster Map\nThe LISA Cluster to which each hexagon belongs can be found in the mean, median, and psyal columns created by local_moran(). This can be used in combination with tmap to create our cluster map. The median will be used for the most accurate representation, accounting for outliers. Similar to before, we will only highlight polygons with a statistically significant value (p_value &lt; 0.05).\nThe four segments of the LISA Cluster Map are similar to those of the Moran Scatterplot:\n\nHigh - High: Areas of high values surrounded by similar neighbors\nLow - Low: Areas of low values surrounded by similar neighbors\nHigh - Low: Areas of high values surrounded by dissimilar neighbors\nLow - High: Areas of low values surrounded by dissimilar neighbors\n\n\ntmap_mode('view')\n\ncluster_palette = c('#0000FF', '#FFB6C1', '#7EC0EE', '#FF0000')\n\n# Weekday morning peak 6am - 9am\nwd69_cluster &lt;- wd69_lisa %&gt;%\n  filter(p_ii &lt;= 0.05) %&gt;%\n  tm_shape()+\n  tm_fill(col = 'mean',\n          title = 'Weekday Morning',\n          palette = cluster_palette)\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nwd1720_cluster &lt;- wd1720_lisa %&gt;%\n  filter(p_ii &lt;= 0.05) %&gt;%\n  tm_shape()+\n  tm_fill(col = 'mean',\n          title = 'Weekday Afternoon',\n          palette = cluster_palette)\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nweh1114_cluster &lt;- weh1114_lisa %&gt;%\n  filter(p_ii &lt;= 0.05) %&gt;%\n  tm_shape()+\n  tm_fill(col = 'mean',\n          title = 'Weekend/holiday Morning',\n          palette = cluster_palette)\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nweh1619_cluster &lt;- weh1619_lisa %&gt;%\n  filter(p_ii &lt;= 0.05) %&gt;%\n  tm_shape()+\n  tm_fill(col = 'mean',\n          title = 'Weekend/holiday Evening',\n          palette = cluster_palette)\n\ntmap_arrange(wd69_cluster, wd1720_cluster, weh1114_cluster, weh1619_cluster, ncol = 2, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue to the display of only statistically significant hexagons, the pattern in the LISA Cluster Map is similar to that of the mapping of Local Moran’s I values and p-value. However, it is possible to observe a that there seems to be an interlacing of Low-High cells next to High-High cells. This means that there are cells with low values surrounded by neighbors with high values next cells with high values surrounded by neighbors with high values. Possibly, this might result from some bus stops in a cell which see very few passengers that are near more frequented bus stops in neighboring cells.\nWhat is more interesting that there are groups of statistically significant cells surrounding a non-significant cells. This is the case, for example, for Tampines Interchange whose cell is not statistically significant by itself. This pattern is similar to those in the previous visualizations as well. However, if the raw trip number is used, one will find that this cell often has the highest number of trips. It is possible that due to its and, potentially, other interchanges’ high trip values due to their special statuses, that they are spatial outliers. However, for their neighbors, their high number of trips help to define their level of spatial autocorrelation in the negative positive direction."
  },
  {
    "objectID": "Personal_Proj/LISA/Take-Home_Ex1.html#conclusion",
    "href": "Personal_Proj/LISA/Take-Home_Ex1.html#conclusion",
    "title": "Analysis of Geospatial Distribution of Bus Ridership by Origin Bus Stop in Singapore",
    "section": "Conclusion",
    "text": "Conclusion\nThis study aimed to conduct exploratory data analysis and analysis of LISA on the number of trips by origin bus stop in Singapore for four time frames: Weekday Morning, Weekday Afternoon, Weekend/holiday Morning, Weekend/holiday Evening. The analysis was done based on a layer of hexagon cells, each consisting of a certain number of bus stops.\nData preparation was done on aspatial and geospatial data sets which were retrieved from LTA Data Mall. For aspatial data, this required separating the bus stop trips by origin bus stops in September 2023 into four different time frames. Subsequently, the number of trips were tallied by the origin bus stops. For geospatial data, hexagon layers were created based on the geometry of the bus stops’ location in Singapore. The hexagons have an edge-to-edge distance of 500 metre. Finally the aspatial and geospatial data sets were combined so that each hexagon would contain a certain number of bus stops and their total number of trips for each of the four time frame.\nExploratory Data Analysis in the form of descriptive analysis and geovisualization shows that each hexagon can contain a wide range of bus trips, due to its number of bus stops as well as the popularity of the bus stops. Additionally, the range of trips in each time frame are different, with higher values tending to be during the Weekday periods. It was also found that there seems to be visual clusters where hexagons with higher number of trips tend to be nearer to each other. This provided the rationale for LISA analysis.\nBefore LISA analysis was conducted, the neighborhood was defined using the Adaptive Distance Matrix, due to the inappropriateness of the Contiguity and Fixed Distance Matrix methods. The neighbor list, weight list using inverse distance matrix, and spatially lagged number of trips were then created for each hexagon for each time frame.\nA null and alternative hypotheses were defined before conducting the LISA analysis. The LISA analysis consisted of calculating the relevant Local Moran’s I statistics for each hexagon for each time frame and visualizing them. In addition, the Moran scatter plot and LISA cluster map were created using the quadrant structure. In order to maintain visual comprehensibility, only hexagons with a statistically significant p-value were visualized. From these analysis, it was found that there were a different pattern of bus stop usage from Weekday Morning to Weekday Afternoon and Weekend/Holiday Noon to Weekend/Holiday Evening as people go and return from work or leisure activities. There were clustering around areas with high number of bus trips, resulting in a number of High-High and Low-High, where less frequented bus stops were neighbors with highly frequented bus stops. Lastly, it was determined that major bus interchanges and stops, which have very high number of trips, could be spatial outliers which are not statistically significant due to their high values, but do affect the spatial relationships of their neighbors.\n\nFuture Works\nFuture studies might consider studying the flow of bus trips in order to have better grasp of the demand during different time frames. This might better explain the effect of bus interchanges, where many bus services terminate and begin.\nAdditionally, the usage of bus stops can be considered in conjunction with the usage of MRT stations as they can work in tandem, especially where bus interchanges are concerned.\nFor future analysis, different types of bus services should be differentiated as feeder bus services are different from long-haul or express bus services, albeit being highly essential to the transportation needs in local communities. This could be done by adding an extra variable to classify the bus stop based on the majority of their bus services."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About this Site",
    "section": "",
    "text": "Hi, my name is Phan Hoang Long and I am a graduate student in the Master of IT in Business (Analytics) program at Singapore Management University (SMU).\nI am an aspiring data analyst with experience in performing analysis in Python, R, and the SAS Viya family of tools. In addition to my academic training in data analytics, I am also trained in Political Science and Public Policy, allowing me to consider problems from both a quantitative and qualitative perspective.\nOver the past year, I have been introduced to a variety of domains in data analytics, from standard business statistical analytics, to textual and geospatial analytics. This website is an attempt to compile some of my individual and group projects to document my own learning, and to serve as a showcase of what I have done and what I can do.\nThis is just a small display of what I have learned. My goal is not to demonstrate that I am fully an expert, but rather than I am always willing to learn and expand my fields of knowledge and capability. My learning will never stop, and I hope to be updating this website as my capacity grows going forward!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contact",
    "section": "",
    "text": "Email: phanlong261@gmail.com\nLinkedIn: linkedin.com/in/phanhlong/"
  },
  {
    "objectID": "Personal_Proj/LISA/data/geospatial/hexagon.html",
    "href": "Personal_Proj/LISA/data/geospatial/hexagon.html",
    "title": "Personal Projects",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n                 +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs 0 0     false"
  },
  {
    "objectID": "Personal_Proj/Fake_News_Detection/fake_news.html",
    "href": "Personal_Proj/Fake_News_Detection/fake_news.html",
    "title": "Untangling Fact from Falsehood Using NLP",
    "section": "",
    "text": "This project was done as part of ISSS609: Text Analytics and Applications. The analysis was performed jointly with Chock Wan Kee, Denise Tan Shi Min, Lim Li Ying, Noel Ng Ser Ying, and Tan Yanni Regine.\nMain language/tool: Python"
  },
  {
    "objectID": "Personal_Proj/Fake_News_Detection/fake_news.html#import-libraries-and-datasets",
    "href": "Personal_Proj/Fake_News_Detection/fake_news.html#import-libraries-and-datasets",
    "title": "Untangling Fact from Falsehood Using NLP",
    "section": "Import Libraries and Datasets",
    "text": "Import Libraries and Datasets\n\nLoad Required Packages\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nimport gensim\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom gensim.models import CoherenceModel\n\nimport importlib\nimport subprocess\nimport re\nimport warnings\nimport datetime\nimport itertools\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_auc_score, roc_curve, make_scorer, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import StandardScaler\n\nfrom scipy import stats\nfrom scipy.sparse import hstack\n\nimport xgboost as xgb\n\nfrom textblob import TextBlob\n\ntry:\n    importlib.import_module('textblob')\nexcept ImportError:\n    subprocess.check_call(['pip', 'install', 'textblob'])\n    \npd.set_option('display.max_columns', 7)\n\n\n\nLoad Dataset\n\ndf_fake = pd.read_csv('data/Fake.csv')\ndf_true = pd.read_csv('data/True.csv')\n\n\n\nSee the Top 5 Rows in Each Dataframe\n\ndf_fake.head()\n\n\n\n\n\n\n\n\ntitle\ntext\nsubject\ndate\n\n\n\n\n0\nDonald Trump Sends Out Embarrassing New Year’...\nDonald Trump just couldn t wish all Americans ...\nNews\nDecember 31, 2017\n\n\n1\nDrunk Bragging Trump Staffer Started Russian ...\nHouse Intelligence Committee Chairman Devin Nu...\nNews\nDecember 31, 2017\n\n\n2\nSheriff David Clarke Becomes An Internet Joke...\nOn Friday, it was revealed that former Milwauk...\nNews\nDecember 30, 2017\n\n\n3\nTrump Is So Obsessed He Even Has Obama’s Name...\nOn Christmas day, Donald Trump announced that ...\nNews\nDecember 29, 2017\n\n\n4\nPope Francis Just Called Out Donald Trump Dur...\nPope Francis used his annual Christmas Day mes...\nNews\nDecember 25, 2017\n\n\n\n\n\n\n\n\ndf_true.head()\n\n\n\n\n\n\n\n\ntitle\ntext\nsubject\ndate\n\n\n\n\n0\nAs U.S. budget fight looms, Republicans flip t...\nWASHINGTON (Reuters) - The head of a conservat...\npoliticsNews\nDecember 31, 2017\n\n\n1\nU.S. military to accept transgender recruits o...\nWASHINGTON (Reuters) - Transgender people will...\npoliticsNews\nDecember 29, 2017\n\n\n2\nSenior U.S. Republican senator: 'Let Mr. Muell...\nWASHINGTON (Reuters) - The special counsel inv...\npoliticsNews\nDecember 31, 2017\n\n\n3\nFBI Russia probe helped by Australian diplomat...\nWASHINGTON (Reuters) - Trump campaign adviser ...\npoliticsNews\nDecember 30, 2017\n\n\n4\nTrump wants Postal Service to charge 'much mor...\nSEATTLE/WASHINGTON (Reuters) - President Donal...\npoliticsNews\nDecember 29, 2017"
  },
  {
    "objectID": "Personal_Proj/Fake_News_Detection/fake_news.html#introduction",
    "href": "Personal_Proj/Fake_News_Detection/fake_news.html#introduction",
    "title": "Untangling Fact from Falsehood Using NLP",
    "section": "Introduction",
    "text": "Introduction\nThe proliferation of fake news is a growing global concern, impacting public opinion, political processes, and trust in media institutions. Fake news is defined as fabricated information that mimics news media content in form but not in organizational process or intent (Lazer, et al., 2018). The spread of misinformation poses significant challenges to foster an informed citizenry as it makes people more susceptible to political misinterpretations (Guess, et al., 2020), thereby increasing the challenge of upholding credibility and integrity of information sources.\nMany experiments have been designed and conducted to put Machine Learning techniques to the test in detecting fake from true news. Our project seeks to determine if detection accuracy can be improved with additional features from sentiment analysis.  Sentiment analysis discerns the emotional tone of text, thereby providing insights to how information is framed and its potential impact on readers.\nThe dataset used in this project is the “Fake News Detection” dataset from Kaggle. It contains fake and real news from 31 May 2015 to 19 Feb 2018. The two classes of data are separated into the respective “Fake” and “True” files, which will be combined for our analysis."
  },
  {
    "objectID": "Personal_Proj/Fake_News_Detection/fake_news.html#methodology",
    "href": "Personal_Proj/Fake_News_Detection/fake_news.html#methodology",
    "title": "Untangling Fact from Falsehood Using NLP",
    "section": "Methodology",
    "text": "Methodology\nWe adopted a sequential approach to the analysis that is summarized in the flow chart below:\n\nFirstly, the separate “Fake” and “True” files had to be merged into one for analysis. To differentiate between the fake and real news after merging for classification, labels were assigned to each record from the same file, 0 for real news and 1 for fake news. After merging both files, the title and text columns are concatenated to create a corpus for text preprocessing.\nNext, preprocessing steps involving tokenization, stop word removal, and stemming are applied on the corpus. It was discovered that some words, such as days of the week and “say” or “said”, appeared in high frequency and did not help to distinguish one document from another. It is reasonable that these words appear frequently in news reports as they provide details on incidents which often include reported speech. In view of the above, we included these words as stop words to be removed from the corpus.\nAfter identifying the best model, additional features such as sentiment analysis scores (including compound score from NLTK’s VADER and subjectivity and polarity scores from TextBlob) were incorporated into the model to evaluate their impact on enhancing the model’s performance. This is due to the fact that sentiment analysis has been shown to be a potentially powerful tool in identifying fake news, as will be elaborated later in the article.\nOne challenge is the size of the corpus containing more than 40,000 documents. Choosing a model that can work efficiently with this corpus size will be a crucial factor.\nFinally, three classification methods are used to find the best performing model in classifying fake news.\nThe following sections will go into the codes used and describe their functions."
  },
  {
    "objectID": "Personal_Proj/Fake_News_Detection/fake_news.html#data-preparation",
    "href": "Personal_Proj/Fake_News_Detection/fake_news.html#data-preparation",
    "title": "Untangling Fact from Falsehood Using NLP",
    "section": "Data Preparation",
    "text": "Data Preparation\nInserting column “class” to identify target features\n\ndf_fake['class'] = 1\ndf_true['class'] = 0\n\nMerging df_true and df_fake\n\ndf_merge = pd.concat([df_true, df_fake], axis = 0)\ndf_merge.head() \n\n\n\n\n\n\n\n\ntitle\ntext\nsubject\ndate\nclass\n\n\n\n\n0\nAs U.S. budget fight looms, Republicans flip t...\nWASHINGTON (Reuters) - The head of a conservat...\npoliticsNews\nDecember 31, 2017\n0\n\n\n1\nU.S. military to accept transgender recruits o...\nWASHINGTON (Reuters) - Transgender people will...\npoliticsNews\nDecember 29, 2017\n0\n\n\n2\nSenior U.S. Republican senator: 'Let Mr. Muell...\nWASHINGTON (Reuters) - The special counsel inv...\npoliticsNews\nDecember 31, 2017\n0\n\n\n3\nFBI Russia probe helped by Australian diplomat...\nWASHINGTON (Reuters) - Trump campaign adviser ...\npoliticsNews\nDecember 30, 2017\n0\n\n\n4\nTrump wants Postal Service to charge 'much mor...\nSEATTLE/WASHINGTON (Reuters) - President Donal...\npoliticsNews\nDecember 29, 2017\n0\n\n\n\n\n\n\n\nLet’s determine the number of articles in the corpus.\n\nprint('The number of articles in the corpus is: ', len(df_merge))\n\nThe number of articles in the corpus is:  44898\n\n\nCombine title and text into one column for later analysis to consider both fields.\n\ndf_merge['text'] = df_merge['title'] + df_merge['text']\ndf_merge.head()\n\n\n\n\n\n\n\n\ntitle\ntext\nsubject\ndate\nclass\n\n\n\n\n0\nAs U.S. budget fight looms, Republicans flip t...\nAs U.S. budget fight looms, Republicans flip t...\npoliticsNews\nDecember 31, 2017\n0\n\n\n1\nU.S. military to accept transgender recruits o...\nU.S. military to accept transgender recruits o...\npoliticsNews\nDecember 29, 2017\n0\n\n\n2\nSenior U.S. Republican senator: 'Let Mr. Muell...\nSenior U.S. Republican senator: 'Let Mr. Muell...\npoliticsNews\nDecember 31, 2017\n0\n\n\n3\nFBI Russia probe helped by Australian diplomat...\nFBI Russia probe helped by Australian diplomat...\npoliticsNews\nDecember 30, 2017\n0\n\n\n4\nTrump wants Postal Service to charge 'much mor...\nTrump wants Postal Service to charge 'much mor...\npoliticsNews\nDecember 29, 2017\n0\n\n\n\n\n\n\n\nThere are columns which we would not need for the purpose of this analysis: title, subject, and date. This can be dropped from the dataframe.\n\ndf = df_merge.drop([\"title\", \"subject\", \"date\"], axis = 1)\ndf.head()\n\n\n\n\n\n\n\n\ntext\nclass\n\n\n\n\n0\nAs U.S. budget fight looms, Republicans flip t...\n0\n\n\n1\nU.S. military to accept transgender recruits o...\n0\n\n\n2\nSenior U.S. Republican senator: 'Let Mr. Muell...\n0\n\n\n3\nFBI Russia probe helped by Australian diplomat...\n0\n\n\n4\nTrump wants Postal Service to charge 'much mor...\n0\n\n\n\n\n\n\n\nThe dataframe rows will be shuffled as during concatenation, the True and Fake classes retained their original location next to each other.\n\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\ndf.head()\n\n\n\n\n\n\n\n\ntext\nclass\n\n\n\n\n0\nBREAKING: GOP Chairman Grassley Has Had Enoug...\n1\n\n\n1\nFailed GOP Candidates Remembered In Hilarious...\n1\n\n\n2\nMike Pence’s New DC Neighbors Are HILARIOUSLY...\n1\n\n\n3\nCalifornia AG pledges to defend birth control ...\n0\n\n\n4\nAZ RANCHERS Living On US-Mexico Border Destroy...\n1\n\n\n\n\n\n\n\nWe can check whether the number of entries in each class is relatively balanced. If they are not, imputation or under-sampling would have to be performed later during machine learning.\n\nsns.countplot(x=\"class\",\n              data = df)\nplt.show()\n\n\n\n\nIt is possible to see that the number of entries in each class is relatively similar, eliminating the need for manipulation later on."
  },
  {
    "objectID": "Personal_Proj/Fake_News_Detection/fake_news.html#text-pre-processing",
    "href": "Personal_Proj/Fake_News_Detection/fake_news.html#text-pre-processing",
    "title": "Untangling Fact from Falsehood Using NLP",
    "section": "Text Pre-processing",
    "text": "Text Pre-processing\nThe text pre-processing steps used in this project are listed below:\nRemoval of words that appear only in “true” or “fake” news\nIt can be observed from a visual check of both datasets that certain words and phrases only appear in either data set, while some words appear in both data sets. These words, as shown in the table below, are removed from the corpus as they do not contribute much to distinguish the documents. For example, hyperlink markers such as “www” appears in many articles and provides no value.\nTokenize words and change to lowercase\nNext, to standardize the words in the corpus, .lower() is used to change all word cases to lower case. Following which, the text is tokenized using NLTK’s word_tokenize() function. This will treat each word as a separate component.\nRemove stop words\nNext, stop words are removed from the text. Stop words are words found in text there are deemed unlikely to be useful in information retrieval. Stop words can be understood as words necessary in the use of language but does not provide value for the analysis. Some common stop words are “the”, “is”, “are”. The STOPWORDS library from Gensim is used for this project as it contains the highest number of stop words (337 stop words).\nStemming\nNext, stemming is applied on the remaining words to normalize text by obtaining only the stem of words. Stemming reduces the variations of a word down to single root. For example, “cats” would simply become “cat”. This is done using the .stem() function from the PorterStemmer module under NLTK.\nRemove punctuation\nNext, we remove all punctuation marks that are retained in the list of words after tokenization. This is achieved by using the .isalpha() function to retain only words that contain only alphabetic characters. Besides removing punctuation points, we can achieve a secondary advantage of removing numeric and special characters that may not contribute meaningfully to the analysis.\nRefining pre-processing steps\nFinally, after an initial round of pre-processing, it was observed that the pre-processed text contained a high frequency of the following: words that are single characters, days of the week, and “says” or “said”. Days of the week and “says” or “said” are words commonly occurring in news articles. Hence, the pre-processing step is refined to include a custom list of stop words (each day of the week, “says”, “said”), and to only retain words that are longer than one character.\nThe code chunk below will perform the described pre-processing steps.\n\n## A function is defined to perform pre-processing steps for easier usage later on\n\ncustom_stop_word = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'said', 'says', 's', 't']\nstop_words = STOPWORDS.union(set(custom_stop_word)) # setting the stopwords\nstemmer = PorterStemmer() # setting the stemmer\n\ndef preprocess_tokens(text):\n    words = re.sub('Reuters', '', text) # removing the word Reuters as it appears in all the real news\n    words = re.sub('https?:\\S+|www\\.\\S+', ' ', words) # remove URLs that begin with 'https' or 'www'\n    words = re.sub('bit\\.ly\\S+', ' ', words) # remove URLS that begin with 'bit.ly' - only appears in true news\n    words = re.sub('pic\\.twitter\\.com\\S+', ' ', words) # remove URLs that begin with 'pic.twitter.com' - only appears in fake news\n    words = word_tokenize(words.lower())  # convert text to lowercase & split into word tokens\n    words = [word for word in words if not word in stop_words] # removing the stop words\n    words = [stemmer.stem(word) for word in words] # stemming the words\n    words = [word for word in words if word.isalpha()] # removing punctuation\n    words = [word for word in words if len(word) &gt; 1] # remove single character words\n    return words\n\nThe preprrocess_token() function can now be applied onto the text column of our dataframe.\n\ndf['text_preprocessed'] = df[\"text\"].apply(preprocess_tokens)\ndf.head()\n\n\n\n\n\n\n\n\ntext\nclass\ntext_preprocessed\n\n\n\n\n0\nBREAKING: GOP Chairman Grassley Has Had Enoug...\n1\n[break, gop, chairman, grassley, demand, trump...\n\n\n1\nFailed GOP Candidates Remembered In Hilarious...\n1\n[fail, gop, candid, rememb, hilari, mock, eulo...\n\n\n2\nMike Pence’s New DC Neighbors Are HILARIOUSLY...\n1\n[mike, penc, new, dc, neighbor, hilari, troll,...\n\n\n3\nCalifornia AG pledges to defend birth control ...\n0\n[california, ag, pledg, defend, birth, control...\n\n\n4\nAZ RANCHERS Living On US-Mexico Border Destroy...\n1\n[az, rancher, live, border, destroy, nanci, pe...\n\n\n\n\n\n\n\nLet’s determine the distribution of lengths of each token using a plot.\n\n# Find the length of each list of preprocessed tokens\nlengths = [len(tokens) for tokens in df['text_preprocessed']]\n\n# Plot the distribution of the length of the list of preprocessed tokens\nplt.figure(figsize=(8, 6))\nsns.histplot(lengths, bins=30, color='skyblue', edgecolor='black', kde=False)\nplt.title('Distribution of the Length of Preprocessed Tokens')\nplt.xlabel('Length of Preprocessed Tokens')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\nAlternatively, it is also possible to determine the top 20 most common tokens in the entire corpus. Due to the large number of articles in the corpus, we can sample just the first 1,000 articles in order to get a glimpse of the top 20 most common tokens.\n\nfirst1000 = list(itertools.chain.from_iterable(df['text_preprocessed'][:1000]))\nprint('most frequent tokens in first 1000 articles \\n', nltk.FreqDist(first1000).most_common(20))\n\nmost frequent tokens in first 1000 articles \n [('trump', 3216), ('presid', 1328), ('state', 1272), ('peopl', 891), ('republican', 808), ('new', 693), ('obama', 692), ('year', 663), ('democrat', 651), ('like', 643), ('hous', 641), ('donald', 627), ('elect', 611), ('white', 610), ('support', 597), ('nation', 583), ('clinton', 572), ('parti', 572), ('report', 563), ('vote', 559)]\n\n\nWe can see that topics related to United States (U.S) politics dominate the top tokens. These include tokens related to the U.S presidents such as Donald Trump or Barack Obama."
  },
  {
    "objectID": "Personal_Proj/Fake_News_Detection/fake_news.html#document-classification---baseline",
    "href": "Personal_Proj/Fake_News_Detection/fake_news.html#document-classification---baseline",
    "title": "Untangling Fact from Falsehood Using NLP",
    "section": "Document Classification - Baseline",
    "text": "Document Classification - Baseline\nAs the project aims to understand the impact of using sentiment analysis to improve the accuracy of fake news classification using machine learning techniques, a baseline classification model is trained using only the processed data that excludes additional features.\nTwo model evaluation methods are explored in this project. The Train-Test Split Method is used and validated using the evaluation metrics, the K-Fold Cross Validation Method is used. As mentioned in the previous section, three classification models are employed in this project: XGBoost, SVM, and Logistic Regression.\nFirst, the processed data is split into 70% for training and 30% for testing. This ratio is decided as it provides sufficient data to train the classification models (31,429 records), and adequate data to evaluate the performance of the models (13,469 records). Furthermore, this split also balances the computational resources required to train the models and leaves enough data for K-Fold Cross Validation.\nNext, the processed data is vectorized using the TfidfVectorizer, which converts raw strings into a matrix of Term Frequency-Inverse Document Frequency (TF-IDF) features. This allows a higher weightage to be assigned to words that occur less frequently in the corpus as these words are more discriminative. The split X_train data is vectorized using the .fit_transform() function while the X_test data is vectorized using the .transform() function.\nNext, we define a function to return the evaluation results for each model. The function makes predictions using the test data and calculates the evaluation metrics. The metrics are calculated using the classification_report() function from sklearn and it includes the precision, recall, f1-score, and support. The function also generates the confusion matrix created using the confusion_matrix() function from sklearn and the AUC-ROC curve, which plots true positive rate against false positive rate. This function can be called with each model to evaluate them separately.\nUsing the train-test split method alone may introduce bias that is an over- or underestimation of the performance of a model. K-fold cross-validation can be used to conduct a systematic evaluation of the model (Kohavi, 1995).\nTo achieve this, we wrote another function to evaluate each model using the k-fold evaluation method, where k = 10 in this project. This is implemented using the StratifiedKFold function from sklearn. This function splits the entire vectorized data set into 10 equal-sized parts with approximately similar proportions of class 0 and class 1 data. Next, we evaluated each model 10 times, each time using one fold as the validation set, and the average of the performance over 10 rounds of training and validation is used as the overall performance metric. This is achieved using the cross_validate() function from sklearn, and the precision, recall, f1-score, and support scores are created using the make_scorer() function from sklearn.\nAfter obtaining the results of the baseline model, the data frame is fitted with features from the subsequent analyses – sentiment analysis using VADER and TextBlob. The train-test split method and k-fold cross-validation are applied to the corpus with the addition of the following features:\n\nCompound score from VADER\nPolarity and Subjectivity scores from TextBlob\nCompound score from VADER, and Polarity and Subjectivity scores from TextBlob\n\nBut first, let’s consider the performance of a simple baseline solution without any additional features. This can be performed using the code chunks below.\n\nSimple Train - Test Model Evaluation\nIn this section, our baseline solution will only split the data into a train and test set in order to evaluate the performance of each model.\nFirst, we define variables to store the independent variable, or the text, and the dependent variable, or the class of Fake or True.\n\nX = df['text_preprocessed']\ny = df['class']\n\nNext, we split the data into a train and test set in order to make sure the models can be trained and evaluated on separate data sets, or in other words, to make sure that each model is not being tested on data it has seen during training.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)\n\nBecause the TfidfVectorizer will not work with word tokens, our tokens would need to be passed through a dummy function first. Essentially, the TfidfVectorizer has its own tokenizer argument meant to tokenize input strings. By inputing our dummy function as a tokenizer, we can use the tokens we have already created.\n\ndef dummy(tokens):\n    return tokens\n\nvectorizer = TfidfVectorizer(tokenizer=dummy, \n                             preprocessor=dummy,\n                             token_pattern=None, \n                             ngram_range = (1,1), \n                             min_df = 0.1)\n                             \nX_train = vectorizer.fit_transform(X_train)\nX_test = vectorizer.transform(X_test)\n\nNext, we can create a dictionary of models we would like to evaluate. For this analysis, we will use three models discussed during the course: XGBoost, Support Vector Machine (SVM), and Logistic Regression.\n\nmodels = {\n    'XGBoost': xgb.XGBClassifier(),\n    'SVM': CalibratedClassifierCV(svm.LinearSVC()),\n    'LogReg': LogisticRegression(class_weight='balanced')\n}\n\nA function called model_evaluate is defined in order to evaluate each model and return evaluations in terms of the visualization of a confusion matrix and classification report of different metrics (F1, accuracy, precision, recall). Additionally, an ROC curve is drawn to visually evaluate the performance of the models.\nmodel_evaluate will take each model from the pre-defined dictionary, fit the training data in X_test to it, and then evaluate its performance on the test corpus in X_test.\n\ndef model_evaluate(models, model_name):\n    # Create the pipeline\n    pipeline = Pipeline(steps=[\n        ('model', models[model_name])\n    ])\n\n    # Fit the pipeline on the training data\n    pipeline.fit(X_train, y_train)\n\n    # Make predictions on the test data\n    y_pred = pipeline.predict(X_test)\n     \n    # Calculate evaluation metrics\n    print('Summary report for {}'.format(model_name))\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n\n    # Confusion Matrix\n    print(\"Confusion Matrix:\")\n    cm = confusion_matrix(y_test, y_pred)\n    cmplot = ConfusionMatrixDisplay(cm)\n    cmplot.plot()\n    plt.show()\n\n    # AUC-ROC Curve\n    print(\"AUC-ROC Curve:\")\n    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n    auc_roc = roc_auc_score(y_test, y_pred_proba)\n    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n\n    # Plot the ROC curve\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.4f)' % auc_roc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\nThe evaluation of the XGBoost model:\n\nmodel_evaluate(models,'XGBoost')\n\nSummary report for XGBoost\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96      6448\n           1       0.96      0.96      0.96      7022\n\n    accuracy                           0.96     13470\n   macro avg       0.96      0.96      0.96     13470\nweighted avg       0.96      0.96      0.96     13470\n\nConfusion Matrix:\nAUC-ROC Curve:\n\n\n\n\n\n\n\n\nThe evaluation of the SVM model:\n\nmodel_evaluate(models,'SVM')\n\nSummary report for SVM\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.93      0.92      6448\n           1       0.94      0.91      0.93      7022\n\n    accuracy                           0.92     13470\n   macro avg       0.92      0.92      0.92     13470\nweighted avg       0.92      0.92      0.92     13470\n\nConfusion Matrix:\nAUC-ROC Curve:\n\n\n\n\n\n\n\n\nThe evaluation of the Logistic Regression model:\n\nmodel_evaluate(models,'LogReg')\n\nSummary report for LogReg\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.94      0.92      6448\n           1       0.94      0.91      0.92      7022\n\n    accuracy                           0.92     13470\n   macro avg       0.92      0.92      0.92     13470\nweighted avg       0.92      0.92      0.92     13470\n\nConfusion Matrix:\nAUC-ROC Curve:\n\n\n\n\n\n\n\n\nBased on the evaluation of the three models, the XGBoost model can be considered to be the most effective as it achieved the highest F1-score and the highest accuracy of 96%. In order to improve the rigor of our machine learning, K-fold Cross Validation can be performed to further evaluate each of the model.\n\n\nK-fold Cross Validation Method\nIt is possible to tweak our functions to perform StratifiedKFold validation instead of a simple train-test split.\n\nscoring = {'accuracy' : make_scorer(accuracy_score), \n           'precision' : make_scorer(precision_score),\n           'recall' : make_scorer(recall_score), \n           'f1_score' : make_scorer(f1_score)}\n           \ndef model_evaluate_kfold(models, model_name):\n    pipeline = Pipeline(steps=[\n        ('vectorizer', TfidfVectorizer(tokenizer=dummy, preprocessor=dummy, token_pattern=None, ngram_range=(1, 1), min_df=0.1)),\n        ('model', models[model_name])\n    ])\n\n    # Use stratified k-fold cross-validation\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\n    # Perform cross-validation\n    scores = cross_validate(pipeline, X, y, cv=cv, scoring=scoring)\n    print(f\"Cross-validation scores for {model_name}:\")\n    print(f\"Accuracy: {scores['test_accuracy']}\")\n    print(f\"Precision: {scores['test_precision'].mean()}\")\n    print(f\"Recall: {scores['test_recall'].mean()}\")\n    print(f\"F1-score: {scores['test_f1_score'].mean()}\")\n    print(f\"Mean accuracy: {scores['test_accuracy'].mean()}\")\n    print(f\"Standard deviation: {scores['test_accuracy'].std()}\")\n\nNow, it is possible to evaluate our models, starting with XGBoost. Due to the nature of k-fold validation, the process would take longer than simple train-test evaluation:\n\nmodel_evaluate_kfold(models,'XGBoost')\n\nCross-validation scores for XGBoost:\nAccuracy: [0.96592428 0.96436526 0.96013363 0.96035635 0.96124722 0.96792873\n 0.96681514 0.9701559  0.96435732 0.96435732]\nPrecision: 0.9645687925543933\nRecall: 0.9678037629554204\nF1-score: 0.9661795229357942\nMean accuracy: 0.9645641139117099\nStandard deviation: 0.0031299726702493265\n\n\nIt is possible to see that the Accuracy and F1-score improved by almost one percentage point when k-fold validation is performed instead of a simple train-test training.\nLet’s continue to evaluate the SVM and Logistic Regression models.\n\nmodel_evaluate_kfold(models,'SVM')\n\nCross-validation scores for SVM:\nAccuracy: [0.9247216  0.92249443 0.92672606 0.91737194 0.9247216  0.93006682\n 0.92227171 0.922049   0.92537313 0.91713076]\nPrecision: 0.9358082510351204\nRecall: 0.916187521893038\nF1-score: 0.9258872932004186\nMean accuracy: 0.9232927061001874\nStandard deviation: 0.0037727925841135918\n\n\n\nmodel_evaluate_kfold(models,'LogReg')\n\nCross-validation scores for LogReg:\nAccuracy: [0.922049   0.92227171 0.92583519 0.91915367 0.92405345 0.92806236\n 0.91937639 0.91982183 0.92269993 0.91735353]\nPrecision: 0.9404459114087015\nRecall: 0.9085218038340285\nF1-score: 0.9242044200936856\nMean accuracy: 0.9220677072040985\nStandard deviation: 0.0031227689149698145\n\n\nIt is possible to detect a slight increase in each evaluation metric for the three models. Moving forward, it can be determined that XGBoost and k-fold cross validation can be used for further experimentation. The baseline results using XGBoost can be stored for later comparison.\n\npipeline = Pipeline(steps=[\n    ('vectorizer', TfidfVectorizer(tokenizer=dummy, preprocessor=dummy, token_pattern=None, ngram_range=(1, 1), min_df=0.1)),\n    ('model', models['XGBoost'])\n])\n\n# Use stratified k-fold cross-validation\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\n# Perform cross-validation\nscores = cross_validate(pipeline, X, y, cv=cv, scoring=scoring)\nscores_baseline = [scores['test_f1_score'].mean(), scores['test_accuracy'].mean()]\n\nFollowing the determination of a model and evaluation method, Sentiment Analysis can be performed in order to understand the sentiments of fake news versus real news and to potentially create new features for fake news detection with machine learning."
  },
  {
    "objectID": "Personal_Proj/Fake_News_Detection/fake_news.html#sentiment-analysis",
    "href": "Personal_Proj/Fake_News_Detection/fake_news.html#sentiment-analysis",
    "title": "Untangling Fact from Falsehood Using NLP",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nSentiment analysis, or opinion mining, aims to identify the emotional pattern within written text. By conducting sentiment analysis, the sentiment, attitudes, and emotions expressed by a piece of text can be determined (Liu, 2020). Such analysis is particularly relevant to the fake news detection as targeting emotions and provoking an emotional response are the main vectors of effectiveness and spread for fraudulent information (Bakir & McStay, 2018; Horner, et al., 2021). In addition, due to the prominent role that sentiments play in fake news, multiple studies have found that by including sentiment analysis, the performance of a fake news classifier can be improved (Ajao et al., 2019; Alonso et al., 2021; Zhang et al., 2021).\nIn this project, the goal of sentiment analysis is two-fold. First, sentiment analysis is an objective in and of itself, allowing the study to capture any emotional disparity between fake and genuine news and describe the usage of emotions in fake news. Second, the results of sentiment analysis can serve as new features for document classification, potentially improving its performance. We utilized two of the most popular Python packages, NLTK and TextBlob. However, it is important to note that a clear drawback of any sentiment analysis task based on existing dictionaries will be that the context of the dictionary might not match with the context of the current corpus.\nFor the purpose of this project, sentiment analysis will be performed using two libraries, NLTK’s VADER and TextBlob.\n\nVADER\nNLTK is a powerful tool for natural language processing, including classifiers and sentiment lexicons for sentiment analysis. It works with many text analytics tasks, provides users with a pretrained lexicon and rule-based sentiment analyzer called Valence Aware Dictionary and sEntiment Reasoner (VADER) (Mogyorosi, n.d.). The VADER analyzer uses the vader_lexicon developed by C.J. Hutto and Eric Gilbert, which measures a sentiment intensity score from words, phrases and emojis, and based on their emotional content, from negative to positive. This vader_lexicon can be applied to a piece of written text using NLTK’s SentimentIntensityAnalyzer’s polarity_score() function. SentimentIntensityAnalyzer can be applied to whole sentences instead of tokenized text. However, we note that VADER was trained specifically for social media data to handle complex language sentiments, which may not be completely applicable to fake news.\nAs the VADER lexicon can be applied directly to text using the SentimentIntensityAnalyzer(), polarity scores for each document in the corpus can be generated by applying .polarity_score() on each record of the processed text dataframe, df. Four score types can be generated from .polarity_score(), and we store each score in one newly created column in df:\n\nCompound: overall sentiment score between -1 to 1\nNegative: negative sentiment score between 0 to 1\nNeutral: neutral sentiment score between 0 to 1\nPositive: positive sentiment score between 0 to 1\n\nThe compound score is used as a feature in the classification model as it represents the overall sentiment of each document. The compound score is the sum of the valence scores of each word in the lexicon, normalized to be between -1 and +1 (Swarnkar, 2020).\nFirst, we load the SentimentIntensityAnalyzer into a variable.\n\nsen_ana = SentimentIntensityAnalyzer()\n\nLet’s test how the results of the analyzer are displayed.\n\nprint(sen_ana.polarity_scores('I love quiz'))\nprint(sen_ana.polarity_scores('I hate quiz'))\n\n{'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n{'neg': 0.787, 'neu': 0.213, 'pos': 0.0, 'compound': -0.5719}\n\n\nWe can see that the first statement is rated to be very positive while the second statement is considered very negative, which makes sense with a cursory inspection.\nInstead of having all four scores combined as above, it would be easier for analysis if they’re stored in separate columns. To do so, we can assign each of the score to a new column in the data frame.\n\ndf['nltk_compound']  = df['text'].apply(lambda x: sen_ana.polarity_scores(x)['compound'])\ndf['nltk_neg'] = df['text'].apply(lambda x: sen_ana.polarity_scores(x)['neg'])\ndf['nltk_neu'] = df['text'].apply(lambda x: sen_ana.polarity_scores(x)['neu'])\ndf['nltk_pos'] = df['text'].apply(lambda x: sen_ana.polarity_scores(x)['pos'])\n\nIt is now possible to see the average sentiment scores by class of articles.\n\ndf.groupby('class')[['nltk_neg','nltk_neu','nltk_pos','nltk_compound']].mean()\n\n\n\n\n\n\n\n\nnltk_neg\nnltk_neu\nnltk_pos\nnltk_compound\n\n\nclass\n\n\n\n\n\n\n\n\n0\n0.078701\n0.839465\n0.081833\n0.055346\n\n\n1\n0.101657\n0.805650\n0.092697\n-0.109791\n\n\n\n\n\n\n\nIt can be observed that the sentiment differences between the two classes of fake and true news are not significantly divergent based on their mean scores. A visualization might help to further elucidate any potential differences.\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\nsns.histplot(df, x=\"nltk_neg\", hue=\"class\", stat=\"count\",ax=axs[0, 0])\nsns.histplot(df, x=\"nltk_neu\", hue=\"class\", stat=\"count\",ax=axs[0, 1])\nsns.histplot(df, x=\"nltk_pos\", hue=\"class\", stat=\"count\",ax=axs[1, 0])\nsns.histplot(df, x=\"nltk_compound\", hue=\"class\", stat=\"count\",ax=axs[1, 1])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nTextBlob\nTextBlob is a user-friendly library built on NLTK library, allowing users to easily analyze a text corpus. It is more oriented towards user reviews and based on the “pattern” library, which contains high frequency adjectives in reviews. In addition to a customizable library of adjectives, TextBlob also provides a library created through machine learning, namely a Naïve Bayes Classifier model to classify text as positive, negative, or neutral. This library applied a machine-learning model on a corpus of movie reviews and score rating to derive the ratings of words (Loria, 2020).\nThe TextBlob().sentiment function is applied directly to each record of the processed text dataframe, df. Two score types are generated from this function, and each is stored in one newly created column in df:\n\nPolarity: measures negative or positive sentiment between -1 and 1, with -1 defined as a negative sentiment and 1 defined as a positive sentiment.\nSubjectivity: quantifies the amount of personal opinion and factual information contained in the text between 0 and 1. Higher subjectivity means that the text contains personal opinion than facts\n\nLet’s see a demonstration of TextBlob in Python.\n\ntexts = ['I love quiz', 'I hate quiz']\n\n# Create TextBlob objects with the text\n# Analyze sentiments and print\nfor text in texts:\n    sentiment = TextBlob(text).sentiment\n    print(f\"Sentiment for '{text}': {sentiment}\")\n\nSentiment for 'I love quiz': Sentiment(polarity=0.5, subjectivity=0.6)\nSentiment for 'I hate quiz': Sentiment(polarity=-0.8, subjectivity=0.9)\n\n\nSimilar to VADER, TextBlob identified the first statement as having a positive polarity score while the second as having a negative polarity score. It is important to note that even though a cursory examination shows both statements to be equally subjective, TextBlob indicates that one is more subjective than the other.\nTextBlob can be applied to our corpus and the resulting scores can be stored in their own respective columns.\n\ndf['textblob_sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment)\n\n\ndf['textblob_polarity'] = df['textblob_sentiment'].apply(lambda x: x.polarity)\ndf['textblob_subjectivity'] = df['textblob_sentiment'].apply(lambda x: x.subjectivity)\n\nNext, the mean polarity and subjectivity scores from TextBlob can be calculated.\n\nsentiment_means = df.groupby('class')[['textblob_polarity', 'textblob_subjectivity']].mean()\nprint(sentiment_means)\n\n       textblob_polarity  textblob_subjectivity\nclass                                          \n0               0.053636               0.360700\n1               0.056655               0.452103\n\n\nOnce again, the difference between the Fake and True news classes do not seem to be significant. It might be useful to visualize the distribution of scores in order to confirm this observation.\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 8))\nsns.histplot(df, x=\"textblob_polarity\", hue=\"class\", stat=\"count\", ax = axs[0])\nsns.histplot(df, x=\"textblob_subjectivity\", hue=\"class\", stat=\"count\", ax = axs[1])\nplt.tight_layout()\nplt.show()\n\n\n\n\nLet’s combine all the scores into one table for easier comparison.\n\nnltk_sentiment = df.groupby('class')[['nltk_neg','nltk_neu','nltk_pos','nltk_compound']].mean()\ntextblob_sentiment = df.groupby('class')[['textblob_polarity', 'textblob_subjectivity']].mean()\nsummary_table = pd.concat([nltk_sentiment,textblob_sentiment],axis = 1)\nsummary_table.index = ['True','Fake']\nsummary_table\n\n\n\n\n\n\n\n\nnltk_neg\nnltk_neu\nnltk_pos\nnltk_compound\ntextblob_polarity\ntextblob_subjectivity\n\n\n\n\nTrue\n0.078701\n0.839465\n0.081833\n0.055346\n0.053636\n0.360700\n\n\nFake\n0.101657\n0.805650\n0.092697\n-0.109791\n0.056655\n0.452103\n\n\n\n\n\n\n\nIt can be seen that Fake News have an NLTK Negative mean score closer to –1 (0.101657), indicating that Fake News articles contain more words linked to negative emotions than real true news. However, if TextBlob scores are considered, the Polarity score for True News is slightly smaller, indicating that True News is more negative. Yet, these scores are so similar that it might not indicate a meaningful difference. At the same time, it is possible that Fake News, in general, contain more emotions, either negative or positive, compared to True News. This is reflected in Fake News higher score for NLTK Positive Sentiment (0.092697) compared to True News (0.081833). By encompassing both more negative and positive emotions, the mean TextBlob Polarity score for Fake News can be normalized.\nOn the other hand, the Subjectivity scores exhibit a larger difference between Fake (0.452103) and True News (0.3607), reflecting the potential for Fake News to contain more opinions than facts compared to True News. Histograms visualizing the distribution difference between True and Fake news can be referenced in the Appendix.\nBecause a difference seems to exist between True News and Fake News for different sentiment categories, the idea that sentiment scores can be added as a feature for the classification task is further reinforced.\nIn the following section, the document classification model using XGBoost and k-fold cross validation will be refitted to a set of features including the vectorized text of the corpus as well as the compound sentiment scores from NLTK (Compound Score) and TextBlob (Polarity Score) and subjectivity score from TextBlob (Subjectivity Score). Each of these feature will be added sequentially, then combined together to discern their effect on model performance."
  },
  {
    "objectID": "Personal_Proj/Fake_News_Detection/fake_news.html#document-classification-with-sentiment-analysis",
    "href": "Personal_Proj/Fake_News_Detection/fake_news.html#document-classification-with-sentiment-analysis",
    "title": "Untangling Fact from Falsehood Using NLP",
    "section": "Document Classification with Sentiment Analysis",
    "text": "Document Classification with Sentiment Analysis\n\nDocument Classification with Sentiment Analysis (NLTK VADER)\nFirstly, we can perform document classification using NLTK’s compound score with XGBoost and k-fold cross validation.\nOnly one step is added to the process, which is to use hstack from numpy in order to stack the features column wise after vectorization of text features.\n\n# Define the dependent and independent variables\nx = df['text_preprocessed']\nx_sentiment_compound = df['nltk_compound'].values.reshape(-1,1)\ny = df['class']\n\n\n# Create features with compound sentiment score\ndef dummy(tokens):\n    return tokens\n\nvectorizer = TfidfVectorizer(tokenizer=dummy, \n                             preprocessor=dummy, \n                             token_pattern=None, \n                             ngram_range=(1, 2), \n                             min_df=0.1)\n                             \nx_feature = vectorizer.fit_transform(x)\nx_compound = hstack((x_feature,x_sentiment_compound))\n\n\nscoring = {'accuracy' : make_scorer(accuracy_score), \n           'precision' : make_scorer(precision_score),\n           'recall' : make_scorer(recall_score), \n           'f1_score' : make_scorer(f1_score)}\n\nmodel = xgb.XGBClassifier()\n\n# Use stratified k-fold cross-validation\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\n# Perform cross-validation\nscores = cross_validate(model, x_compound, y, cv=cv, scoring=scoring)\n\nscores_nltk = [scores['test_f1_score'].mean(),scores['test_accuracy'].mean()]\n\nprint(f\"Cross-validation scores for XGBoost:\")\nprint(f\"Accuracy: {scores['test_accuracy']}\")\nprint(f\"Precision: {scores['test_precision'].mean()}\")\nprint(f\"Recall: {scores['test_recall'].mean()}\")\nprint(f\"F1-score: {scores['test_f1_score'].mean()}\")\nprint(f\"Mean accuracy: {scores['test_accuracy'].mean()}\")\nprint(f\"Standard deviation: {scores['test_accuracy'].std()}\")\n\nCross-validation scores for XGBoost:\nAccuracy: [0.96525612 0.96659243 0.96481069 0.95924276 0.96458797 0.97238307\n 0.96859688 0.97238307 0.96680775 0.96658499]\nPrecision: 0.9668479660630778\nRecall: 0.9696350362581343\nF1-score: 0.9682334814359199\nMean accuracy: 0.9667245744485035\nStandard deviation: 0.0036598976506881397\n\n\nIt can be observed that the F1 score and mean accuracy both demonstrate a marginal increase of around 0.01%, showing that adding features related to sentiment analysis could possibly make a difference. The next section will explore the use of TextBlob’s sentiment and subjectivity scores as features and the combined sets of NLTK and TextBlob features.\n\n\nDocument Classification with Sentiment Analysis (TextBlob)\n\n# Define the dependent and independent variables\nx = df['text_preprocessed']\nx_sentiment_polarity = df['textblob_polarity'].values.reshape(-1,1)\nx_sentiment_subjectivity = df['textblob_subjectivity'].values.reshape(-1,1)\ny = df['class']\n\n\n# Create features with polarity and subjectivity sentiment score\ndef dummy(tokens):\n    return tokens\nvectorizer = TfidfVectorizer(tokenizer=dummy, preprocessor=dummy, token_pattern=None, ngram_range=(1, 2), min_df=0.1)\nx_feature = vectorizer.fit_transform(x)\nx_pol_sub = hstack((x_feature, x_sentiment_polarity, x_sentiment_subjectivity))\n\n\nscoring = {'accuracy' : make_scorer(accuracy_score), \n           'precision' : make_scorer(precision_score),\n           'recall' : make_scorer(recall_score), \n           'f1_score' : make_scorer(f1_score)}\n\nmodel = xgb.XGBClassifier()\n\n# Use stratified k-fold cross-validation\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\n# Perform cross-validation\nscores = cross_validate(model, x_pol_sub, y, cv=cv, scoring=scoring)\n\nscores_textblob = [scores['test_f1_score'].mean(),scores['test_accuracy'].mean()]\n\nprint(f\"Cross-validation scores for XGBoost:\")\nprint(f\"Accuracy: {scores['test_accuracy']}\")\nprint(f\"Precision: {scores['test_precision'].mean()}\")\nprint(f\"Recall: {scores['test_recall'].mean()}\")\nprint(f\"F1-score: {scores['test_f1_score'].mean()}\")\nprint(f\"Mean accuracy: {scores['test_accuracy'].mean()}\")\nprint(f\"Standard deviation: {scores['test_accuracy'].std()}\")\n\nCross-validation scores for XGBoost:\nAccuracy: [0.9674833  0.97060134 0.96035635 0.96213808 0.96592428 0.9714922\n 0.96815145 0.97349666 0.97037202 0.96858989]\nPrecision: 0.9672716036051199\nRecall: 0.9714239558244728\nF1-score: 0.9693375411244309\nMean accuracy: 0.9678605559444741\nStandard deviation: 0.003903097924174063\n\n\n\n\nDocument Classification with Sentiment Analysis (NLTK VADER and TextBlob)\n\n# Define the dependent and independent variables\nx = df['text_preprocessed']\nx_sentiment_compound = df['nltk_compound'].values.reshape(-1,1)\nx_sentiment_polarity = df['textblob_polarity'].values.reshape(-1,1)\nx_sentiment_subjectivity = df['textblob_subjectivity'].values.reshape(-1,1)\ny = df['class']\n\n\n# Create features with compound, polarity, subjecctivity scores\ndef dummy(tokens):\n    return tokens\nvectorizer = TfidfVectorizer(tokenizer=dummy, preprocessor=dummy, token_pattern=None, ngram_range=(1, 2), min_df=0.1)\nx_feature = vectorizer.fit_transform(x)\nx_combined = hstack((x_feature, x_sentiment_compound, x_sentiment_polarity, x_sentiment_subjectivity))\n\n\nscoring = {'accuracy' : make_scorer(accuracy_score), \n           'precision' : make_scorer(precision_score),\n           'recall' : make_scorer(recall_score), \n           'f1_score' : make_scorer(f1_score)}\n\nmodel = xgb.XGBClassifier()\n\n# Use stratified k-fold cross-validation\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\n# Perform cross-validation\nscores = cross_validate(model, x_combined, y, cv=cv, scoring=scoring)\n\nscores_nltk_textblob = [scores['test_f1_score'].mean(),scores['test_accuracy'].mean()]\n\nprint(f\"Cross-validation scores for XGBoost:\")\nprint(f\"Accuracy: {scores['test_accuracy']}\")\nprint(f\"Precision: {scores['test_precision'].mean()}\")\nprint(f\"Recall: {scores['test_recall'].mean()}\")\nprint(f\"F1-score: {scores['test_f1_score'].mean()}\")\nprint(f\"Mean accuracy: {scores['test_accuracy'].mean()}\")\nprint(f\"Standard deviation: {scores['test_accuracy'].std()}\")\n\nCross-validation scores for XGBoost:\nAccuracy: [0.96859688 0.97037862 0.96503341 0.96280624 0.96636971 0.97060134\n 0.96837416 0.97305122 0.97104032 0.96769882]\nPrecision: 0.9682607191411726\nRecall: 0.9714238833009515\nF1-score: 0.9698313664439555\nMean accuracy: 0.9683950721412053\nStandard deviation: 0.0029093480197024363\n\n\n\npd.DataFrame([scores_baseline, scores_nltk,scores_textblob,scores_nltk_textblob], columns = ['F1-score','Accuracy'], index = ['Baseline XGBoost','NLTK VADER','TextBlob','Combined'])\n\n\n\n\n\n\n\n\nF1-score\nAccuracy\n\n\n\n\nBaseline XGBoost\n0.966180\n0.964564\n\n\nNLTK VADER\n0.968233\n0.966725\n\n\nTextBlob\n0.969338\n0.967861\n\n\nCombined\n0.969831\n0.968395\n\n\n\n\n\n\n\nAll three models that incorporated sentiment analysis scores (NLTK VADER, TextBlob, and Combined) showed an improvement in model performance compared to the baseline XGBoost model. These findings correlate with the studies cited that sentiment analysis may be a useful feature for enhancing model performance."
  },
  {
    "objectID": "Personal_Proj/Fake_News_Detection/fake_news.html#discussions-and-gap-analysis",
    "href": "Personal_Proj/Fake_News_Detection/fake_news.html#discussions-and-gap-analysis",
    "title": "Untangling Fact from Falsehood Using NLP",
    "section": "Discussions and Gap Analysis",
    "text": "Discussions and Gap Analysis\n\nInclusion of Sentiment Analysis in Document Classification\nThe results of document classification using the results of sentiment analysis as additional features exhibit improvements over base classification models using only text as features. This shows that the consideration of sentiment analysis could be a fruitful endeavor in the identification of fake news. Additionally, it reinforces the idea that fake news tends to have more emotion expressors than regular news to incite an emotional response from readers. What is lacking, however, is a word-sentiment dictionary which might be more appropriate to the context of news articles. Such a dictionary might further improve the results of the classifications by allowing for better calculation of sentiment scores.\n\n\nNamed Entities\nIn news articles, it is very common for named entities to be used that might not be recognized in dictionaries, and with the lack of recognition, also comes the lack of context surrounding the entities. If the model can recognize these entities, this would then further enhance how it differentiates real news from fake news as there is additional information for the model to consider.\n\n\nData Source\nThe Kaggle dataset that was used seems to likely be from the United States. Most of the fake news were related to America’s politics and thus the model has been tuned to pick up keywords along this theme. This means that if we were to apply this model to datasets from around the world, the model might not be as applicable or accurate anymore as the topics identified will likely not be as relevant. For example, news about Trump’s inauguration might not be as big in Singapore, and thus if the model was used as-is to run on Singapore’s news, the results produced would thereby not be as accurate.\n\n\nTechnical Limitations\nAs we were dealing with a large dataset and performing complex NLP tasks, we faced constraints such as computational resources and processing speed issues. Running the code sometimes took more than an hour, due to the nature of the computations that we were running. This was quite inefficient, as we needed to test multiple models and had several revisions of code, resulting in us having to spend time allowing the code to load.\n\n\nDynamic Nature of Fake News\nAs technology evolves, so do the methods of dissemination of fake news. With ever-changing tactics to deceive the masses, static models that are not continually updated will not be able to adapt to any emerging trends. As such, if the model is not regularly refreshed, it will sooner or later become obsolete and inaccurate."
  },
  {
    "objectID": "Personal_Proj/Fake_News_Detection/fake_news.html#future-work-and-conclusion",
    "href": "Personal_Proj/Fake_News_Detection/fake_news.html#future-work-and-conclusion",
    "title": "Untangling Fact from Falsehood Using NLP",
    "section": "Future Work and Conclusion",
    "text": "Future Work and Conclusion\n\nNamed Entity Recognition (NER)\nNamed Entity Recognition (NER) can extract important entities mentioned in news articles, such as the names of politicians, organizations, or locations. By analyzing the entities mentioned in both fake and real news, patterns may emerge that could indicate the credibility or authenticity of the article. Additionally, by analyzing the context surrounding named entities, NER can help detect instances of misinformation or propaganda. For example, if a fake news article mentions a well-known organization or individual in a misleading context, NER can help identify the discrepancy between the entity’s actual role or stance and how it is portrayed in the article.\n\n\nExtension of Data Source Scope\nTo tackle the second issue, a future extension of this project would then be to re-train the model based on local news. This would then help to include more local topics on top of the American news (as Singapore is so interlinked to the rest of the world, American news would not be completely irrelevant, so keeping this would not be completely useless). The accuracy and precision of the model would then be improved, and thereby we will be able to use this model for local news as well.\n\n\nRegular Updating of Dataset\nOur model will need to be updated regularly with more recent articles to ensure that it can continue to stay relevant and continue to identify fake news accurately, even those disseminated with new methods of trickery."
  },
  {
    "objectID": "Personal_Proj/Fake_News_Detection/fake_news.html#references",
    "href": "Personal_Proj/Fake_News_Detection/fake_news.html#references",
    "title": "Untangling Fact from Falsehood Using NLP",
    "section": "References",
    "text": "References\nAjao, O., Bhowmik, D., & Zargari, S. (2019). Sentiment Aware Fake News Detection on Online Social Networks. ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2507–2511. https://doi.org/10.1109/ICASSP.2019.8683170\nBakir, V., & McStay, A. (2018). Fake News and The Economy of Emotions. Digital Journalism, 6:2, 154-175. doi:https://doi.org/10.1080/21670811.2017.1345645\nGuess, A., Lockett, L., Benjamin, L., Montgomery, J., Nyhan, B., & Reifler, J. (2020). “Fake news” may have limited effects on political participation beyond increasing beliefs in false claims. Harvard Kennedy School (HKS) Misinformation Review. doi:https://doi.org/10.37016/mr-2020-004\nHorner, C., Galletta, D., Crawford, J., & Shirsat, A. (2021). Emotions: The Unexplored Fuel of Fake News on Social Media. Journal of Management Information Systems, 38:4, 1039-1066. doi:https://doi.org/10.1080/07421222.2021.1990610\nKohavi, R. (1995). A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. International Joint Conference on Arti, Vol. 14, pp. 1137–1145. doi:http://frostiebek.free.fr/docs/Machine%20Learning/validation-1.pdf\nLazer, D. M., Baum, M., Benkler, Y., Berinsky, A., Greenhill, K., Menczer, F., . . . Zittrain, J. (2018). The science of fake news. Science359, 1094-1096. doi:https://doi.org/10.1126/science.aao2998\nLiu, B. (2020). Introduction. In B. Liu, Sentiment Analysis Mining Opinions, Sentiments, and Emotions (pp. 1-17). Cambridge University Press. doi:https://doi.org/10.1017/9781108639286.002\nSwarnkar, N. (2020, May 21). VADER Sentiment Analysis: A Complete Guide, Algo Trading and More. Retrieved from QuantInsti: https://blog.quantinsti.com/vader-sentiment/\nZhang, X., Cao, J., Li, X., Sheng, Q., Zhong, L., & Shu, K. (2021). Mining Dual Emotion for Fake News Detection. Proceedings of the Web Conference 2021, 3465–3476. https://doi.org/10.1145/3442381.3450004"
  }
]