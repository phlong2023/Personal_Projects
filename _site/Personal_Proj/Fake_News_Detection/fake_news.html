<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Phan Hoang Long’s Personal Site - Untangling Fact from Falsehood Using NLP</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Phan Hoang Long’s Personal Site</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-data-analytics-projects" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Data Analytics Projects</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-data-analytics-projects">    
        <li>
    <a class="dropdown-item" href="../../Personal_Proj/LISA/Take-Home_Ex1.html" rel="" target="">
 <span class="dropdown-text">Analysis of Geospatial Distribution of Bus Ridership by Origin Bus Stop in Singapore</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../Personal_Proj/Fake_News_Detection/fake_news.html" rel="" target="">
 <span class="dropdown-text">Untangling Fact from Falsehood Using NLP</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../Personal_Proj/airline/airline.html" rel="" target="">
 <span class="dropdown-text">IN PROGRESS Develop &amp; Retain Valuable Airline Customers</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">Contact</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology">Methodology</a></li>
  <li><a href="#import-libraries-and-datasets" id="toc-import-libraries-and-datasets" class="nav-link" data-scroll-target="#import-libraries-and-datasets">Import Libraries and Datasets</a>
  <ul class="collapse">
  <li><a href="#load-required-packages" id="toc-load-required-packages" class="nav-link" data-scroll-target="#load-required-packages">Load Required Packages</a></li>
  <li><a href="#load-dataset" id="toc-load-dataset" class="nav-link" data-scroll-target="#load-dataset">Load Dataset</a></li>
  <li><a href="#see-the-top-5-rows-in-each-dataframe" id="toc-see-the-top-5-rows-in-each-dataframe" class="nav-link" data-scroll-target="#see-the-top-5-rows-in-each-dataframe">See the Top 5 Rows in Each Dataframe</a></li>
  </ul></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">Data Preparation</a></li>
  <li><a href="#text-pre-processing" id="toc-text-pre-processing" class="nav-link" data-scroll-target="#text-pre-processing">Text Pre-processing</a></li>
  <li><a href="#document-classification---baseline" id="toc-document-classification---baseline" class="nav-link" data-scroll-target="#document-classification---baseline">Document Classification - Baseline</a>
  <ul class="collapse">
  <li><a href="#simple-train---test-model-evaluation" id="toc-simple-train---test-model-evaluation" class="nav-link" data-scroll-target="#simple-train---test-model-evaluation">Simple Train - Test Model Evaluation</a></li>
  <li><a href="#k-fold-cross-validation-method" id="toc-k-fold-cross-validation-method" class="nav-link" data-scroll-target="#k-fold-cross-validation-method">K-fold Cross Validation Method</a></li>
  </ul></li>
  <li><a href="#sentiment-analysis" id="toc-sentiment-analysis" class="nav-link" data-scroll-target="#sentiment-analysis">Sentiment Analysis</a>
  <ul class="collapse">
  <li><a href="#vader" id="toc-vader" class="nav-link" data-scroll-target="#vader">VADER</a></li>
  <li><a href="#textblob" id="toc-textblob" class="nav-link" data-scroll-target="#textblob">TextBlob</a></li>
  </ul></li>
  <li><a href="#document-classification-with-sentiment-analysis" id="toc-document-classification-with-sentiment-analysis" class="nav-link" data-scroll-target="#document-classification-with-sentiment-analysis">Document Classification with Sentiment Analysis</a>
  <ul class="collapse">
  <li><a href="#document-classification-with-sentiment-analysis-nltk-vader" id="toc-document-classification-with-sentiment-analysis-nltk-vader" class="nav-link" data-scroll-target="#document-classification-with-sentiment-analysis-nltk-vader">Document Classification with Sentiment Analysis (NLTK VADER)</a></li>
  <li><a href="#document-classification-with-sentiment-analysis-textblob" id="toc-document-classification-with-sentiment-analysis-textblob" class="nav-link" data-scroll-target="#document-classification-with-sentiment-analysis-textblob">Document Classification with Sentiment Analysis (TextBlob)</a></li>
  <li><a href="#document-classification-with-sentiment-analysis-nltk-vader-and-textblob" id="toc-document-classification-with-sentiment-analysis-nltk-vader-and-textblob" class="nav-link" data-scroll-target="#document-classification-with-sentiment-analysis-nltk-vader-and-textblob">Document Classification with Sentiment Analysis (NLTK VADER and TextBlob)</a></li>
  </ul></li>
  <li><a href="#discussions-and-gap-analysis" id="toc-discussions-and-gap-analysis" class="nav-link" data-scroll-target="#discussions-and-gap-analysis">Discussions and Gap Analysis</a>
  <ul class="collapse">
  <li><a href="#inclusion-of-sentiment-analysis-in-document-classification" id="toc-inclusion-of-sentiment-analysis-in-document-classification" class="nav-link" data-scroll-target="#inclusion-of-sentiment-analysis-in-document-classification"><strong>Inclusion of Sentiment Analysis in Document Classification</strong></a></li>
  <li><a href="#named-entities" id="toc-named-entities" class="nav-link" data-scroll-target="#named-entities"><strong>Named Entities</strong></a></li>
  <li><a href="#data-source" id="toc-data-source" class="nav-link" data-scroll-target="#data-source"><strong>Data Source</strong></a></li>
  <li><a href="#technical-limitations" id="toc-technical-limitations" class="nav-link" data-scroll-target="#technical-limitations"><strong>Technical Limitations</strong></a></li>
  <li><a href="#dynamic-nature-of-fake-news" id="toc-dynamic-nature-of-fake-news" class="nav-link" data-scroll-target="#dynamic-nature-of-fake-news"><strong>Dynamic Nature of Fake News</strong></a></li>
  </ul></li>
  <li><a href="#future-work-and-conclusion" id="toc-future-work-and-conclusion" class="nav-link" data-scroll-target="#future-work-and-conclusion">Future Work and Conclusion</a>
  <ul class="collapse">
  <li><a href="#named-entity-recognition-ner" id="toc-named-entity-recognition-ner" class="nav-link" data-scroll-target="#named-entity-recognition-ner"><strong>Named Entity Recognition (NER)</strong></a></li>
  <li><a href="#extension-of-data-source-scope" id="toc-extension-of-data-source-scope" class="nav-link" data-scroll-target="#extension-of-data-source-scope"><strong>Extension of Data Source Scope</strong></a></li>
  <li><a href="#regular-updating-of-dataset" id="toc-regular-updating-of-dataset" class="nav-link" data-scroll-target="#regular-updating-of-dataset"><strong>Regular Updating of Dataset</strong></a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><strong>References</strong></a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Untangling Fact from Falsehood Using NLP</h1>
</div>



<div class="quarto-title-meta">

    
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">August 30, 2024</p>
    </div>
  </div>
    
  </div>
  

</header>

<p>This project was done as part of ISSS609: Text Analytics and Applications. The analysis was performed jointly with Chock Wan Kee, Denise Tan Shi Min, Lim Li Ying, Noel Ng Ser Ying, and Tan Yanni Regine.</p>
<p>Main language/tool: Python</p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The proliferation of fake news is a growing global concern, impacting public opinion, political processes, and trust in media institutions. Fake news is defined as fabricated information that mimics news media content in form but not in organizational process or intent (Lazer, et al., 2018). The spread of misinformation poses significant challenges to foster an informed citizenry as it makes people more susceptible to political misinterpretations (Guess, et al., 2020), thereby increasing the challenge of upholding credibility and integrity of information sources.</p>
<p>Many experiments have been designed and conducted to put Machine Learning techniques to the test in detecting fake from true news. Our project seeks to determine if detection accuracy can be improved with additional features from sentiment analysis. &nbsp;Sentiment analysis discerns the emotional tone of text, thereby providing insights to how information is framed and its potential impact on readers.</p>
<p>The dataset used in this project is the “<a href="https://www.kaggle.com/code/therealsampat/fake-news-detection/input?select=True.csv">Fake News Detection</a>” dataset from Kaggle. It contains fake and real news from 31 May 2015 to 19 Feb 2018. The two classes of data are separated into the respective “Fake” and “True” files, which will be combined for our analysis.</p>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">Methodology</h2>
<p>We adopted a sequential approach to the analysis that is summarized in the flow chart below:</p>
<p><img src="images/clipboard-418630220.png" class="img-fluid"></p>
<p>Firstly, the separate “Fake” and “True” files had to be merged into one for analysis. To differentiate between the fake and real news after merging for classification, labels were assigned to each record from the same file, 0 for real news and 1 for fake news. After merging both files, the title and text columns are concatenated to create a corpus for text preprocessing.</p>
<p>Next, preprocessing steps involving tokenization, stop word removal, and stemming are applied on the corpus. It was discovered that some words, such as days of the week and “say” or “said”, appeared in high frequency and did not help to distinguish one document from another. It is reasonable that these words appear frequently in news reports as they provide details on incidents which often include reported speech. In view of the above, we included these words as stop words to be removed from the corpus.</p>
<p>After identifying the best model, additional features such as sentiment analysis scores (including compound score from NLTK’s VADER and subjectivity and polarity scores from TextBlob) were incorporated into the model to evaluate their impact on enhancing the model’s performance. This is due to the fact that sentiment analysis has been shown to be a potentially powerful tool in identifying fake news, as will be elaborated later in the article.</p>
<p>One challenge is the size of the corpus containing more than 40,000 documents. Choosing a model that can work efficiently with this corpus size will be a crucial factor.</p>
<p>Finally, three classification methods are used to find the best performing model in classifying fake news.</p>
<p>The following sections will go into the codes used and describe their functions.</p>
</section>
<section id="import-libraries-and-datasets" class="level2">
<h2 class="anchored" data-anchor-id="import-libraries-and-datasets">Import Libraries and Datasets</h2>
<section id="load-required-packages" class="level3">
<h3 class="anchored" data-anchor-id="load-required-packages">Load Required Packages</h3>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem.porter <span class="im">import</span> PorterStemmer</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.sentiment <span class="im">import</span> SentimentIntensityAnalyzer</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.parsing.preprocessing <span class="im">import</span> STOPWORDS</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> CoherenceModel</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> importlib</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> subprocess</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datetime</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>warnings.simplefilter(action<span class="op">=</span><span class="st">'ignore'</span>, category<span class="op">=</span><span class="pp">FutureWarning</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> svm</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, cross_validate, StratifiedKFold</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer, CountVectorizer</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_auc_score, roc_curve, make_scorer, accuracy_score, precision_score, recall_score, f1_score</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.calibration <span class="im">import</span> CalibratedClassifierCV</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> LatentDirichletAllocation</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.utils <span class="im">import</span> resample</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.sparse <span class="im">import</span> hstack</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> xgboost <span class="im">as</span> xgb</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> textblob <span class="im">import</span> TextBlob</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    importlib.import_module(<span class="st">'textblob'</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">ImportError</span>:</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    subprocess.check_call([<span class="st">'pip'</span>, <span class="st">'install'</span>, <span class="st">'textblob'</span>])</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="dv">7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="load-dataset" class="level3">
<h3 class="anchored" data-anchor-id="load-dataset">Load Dataset</h3>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>df_fake <span class="op">=</span> pd.read_csv(<span class="st">'data/Fake.csv'</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>df_true <span class="op">=</span> pd.read_csv(<span class="st">'data/True.csv'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="see-the-top-5-rows-in-each-dataframe" class="level3">
<h3 class="anchored" data-anchor-id="see-the-top-5-rows-in-each-dataframe">See the Top 5 Rows in Each Dataframe</h3>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>df_fake.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">subject</th>
<th data-quarto-table-cell-role="th">date</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Donald Trump Sends Out Embarrassing New Year’...</td>
<td>Donald Trump just couldn t wish all Americans ...</td>
<td>News</td>
<td>December 31, 2017</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Drunk Bragging Trump Staffer Started Russian ...</td>
<td>House Intelligence Committee Chairman Devin Nu...</td>
<td>News</td>
<td>December 31, 2017</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Sheriff David Clarke Becomes An Internet Joke...</td>
<td>On Friday, it was revealed that former Milwauk...</td>
<td>News</td>
<td>December 30, 2017</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Trump Is So Obsessed He Even Has Obama’s Name...</td>
<td>On Christmas day, Donald Trump announced that ...</td>
<td>News</td>
<td>December 29, 2017</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Pope Francis Just Called Out Donald Trump Dur...</td>
<td>Pope Francis used his annual Christmas Day mes...</td>
<td>News</td>
<td>December 25, 2017</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df_true.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">subject</th>
<th data-quarto-table-cell-role="th">date</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>As U.S. budget fight looms, Republicans flip t...</td>
<td>WASHINGTON (Reuters) - The head of a conservat...</td>
<td>politicsNews</td>
<td>December 31, 2017</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>U.S. military to accept transgender recruits o...</td>
<td>WASHINGTON (Reuters) - Transgender people will...</td>
<td>politicsNews</td>
<td>December 29, 2017</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>
<td>WASHINGTON (Reuters) - The special counsel inv...</td>
<td>politicsNews</td>
<td>December 31, 2017</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>FBI Russia probe helped by Australian diplomat...</td>
<td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>
<td>politicsNews</td>
<td>December 30, 2017</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Trump wants Postal Service to charge 'much mor...</td>
<td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>
<td>politicsNews</td>
<td>December 29, 2017</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
</section>
<section id="data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation">Data Preparation</h2>
<p>Inserting column “class” to identify target features</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df_fake[<span class="st">'class'</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>df_true[<span class="st">'class'</span>] <span class="op">=</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Merging df_true and df_fake</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>df_merge <span class="op">=</span> pd.concat([df_true, df_fake], axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>df_merge.head() </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">subject</th>
<th data-quarto-table-cell-role="th">date</th>
<th data-quarto-table-cell-role="th">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>As U.S. budget fight looms, Republicans flip t...</td>
<td>WASHINGTON (Reuters) - The head of a conservat...</td>
<td>politicsNews</td>
<td>December 31, 2017</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>U.S. military to accept transgender recruits o...</td>
<td>WASHINGTON (Reuters) - Transgender people will...</td>
<td>politicsNews</td>
<td>December 29, 2017</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>
<td>WASHINGTON (Reuters) - The special counsel inv...</td>
<td>politicsNews</td>
<td>December 31, 2017</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>FBI Russia probe helped by Australian diplomat...</td>
<td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>
<td>politicsNews</td>
<td>December 30, 2017</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Trump wants Postal Service to charge 'much mor...</td>
<td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>
<td>politicsNews</td>
<td>December 29, 2017</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Let’s determine the number of articles in the corpus.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The number of articles in the corpus is: '</span>, <span class="bu">len</span>(df_merge))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The number of articles in the corpus is:  44898</code></pre>
</div>
</div>
<p>Combine title and text into one column for later analysis to consider both fields.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>df_merge[<span class="st">'text'</span>] <span class="op">=</span> df_merge[<span class="st">'title'</span>] <span class="op">+</span> df_merge[<span class="st">'text'</span>]</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>df_merge.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">subject</th>
<th data-quarto-table-cell-role="th">date</th>
<th data-quarto-table-cell-role="th">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>As U.S. budget fight looms, Republicans flip t...</td>
<td>As U.S. budget fight looms, Republicans flip t...</td>
<td>politicsNews</td>
<td>December 31, 2017</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>U.S. military to accept transgender recruits o...</td>
<td>U.S. military to accept transgender recruits o...</td>
<td>politicsNews</td>
<td>December 29, 2017</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>
<td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>
<td>politicsNews</td>
<td>December 31, 2017</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>FBI Russia probe helped by Australian diplomat...</td>
<td>FBI Russia probe helped by Australian diplomat...</td>
<td>politicsNews</td>
<td>December 30, 2017</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Trump wants Postal Service to charge 'much mor...</td>
<td>Trump wants Postal Service to charge 'much mor...</td>
<td>politicsNews</td>
<td>December 29, 2017</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>There are columns which we would not need for the purpose of this analysis: title, subject, and date. This can be dropped from the dataframe.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df_merge.drop([<span class="st">"title"</span>, <span class="st">"subject"</span>, <span class="st">"date"</span>], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>As U.S. budget fight looms, Republicans flip t...</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>U.S. military to accept transgender recruits o...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>FBI Russia probe helped by Australian diplomat...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Trump wants Postal Service to charge 'much mor...</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The dataframe rows will be shuffled as during concatenation, the True and Fake classes retained their original location next to each other.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.sample(frac<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>BREAKING: GOP Chairman Grassley Has Had Enoug...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Failed GOP Candidates Remembered In Hilarious...</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Mike Pence’s New DC Neighbors Are HILARIOUSLY...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>California AG pledges to defend birth control ...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>AZ RANCHERS Living On US-Mexico Border Destroy...</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>We can check whether the number of entries in each class is relatively balanced. If they are not, imputation or under-sampling would have to be performed later during machine learning.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>sns.countplot(x<span class="op">=</span><span class="st">"class"</span>,</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>              data <span class="op">=</span> df)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="fake_news_files/figure-html/cell-12-output-1.png" width="610" height="429"></p>
</div>
</div>
<p>It is possible to see that the number of entries in each class is relatively similar, eliminating the need for manipulation later on.</p>
</section>
<section id="text-pre-processing" class="level2">
<h2 class="anchored" data-anchor-id="text-pre-processing">Text Pre-processing</h2>
<p>The text pre-processing steps used in this project are listed below:</p>
<p><u>Removal of words that appear only in “true” or “fake” news</u></p>
<p>It can be observed from a visual check of both datasets that certain words and phrases only appear in either data set, while some words appear in both data sets. These words, as shown in the table below, are removed from the corpus as they do not contribute much to distinguish the documents. For example, hyperlink markers such as “www” appears in many articles and provides no value.</p>
<p><u>Tokenize words and change to lowercase</u></p>
<p>Next, to standardize the words in the corpus, .lower() is used to change all word cases to lower case. Following which, the text is tokenized using NLTK’s word_tokenize() function. This will treat each word as a separate component.</p>
<p><u>Remove stop words</u></p>
<p>Next, stop words are removed from the text. Stop words are words found in text there are deemed unlikely to be useful in information retrieval. Stop words can be understood as words necessary in the use of language but does not provide value for the analysis. Some common stop words are “the”, “is”, “are”. The STOPWORDS library from Gensim is used for this project as it contains the highest number of stop words (337 stop words).</p>
<p><u>Stemming</u></p>
<p>Next, stemming is applied on the remaining words to normalize text by obtaining only the stem of words. Stemming reduces the variations of a word down to single root. For example, “cats” would simply become “cat”. This is done using the .stem() function from the PorterStemmer module under NLTK.</p>
<p><u>Remove punctuation</u></p>
<p>Next, we remove all punctuation marks that are retained in the list of words after tokenization. This is achieved by using the .isalpha() function to retain only words that contain only alphabetic characters. Besides removing punctuation points, we can achieve a secondary advantage of removing numeric and special characters that may not contribute meaningfully to the analysis.</p>
<p><u>Refining pre-processing steps</u></p>
<p>Finally, after an initial round of pre-processing, it was observed that the pre-processed text contained a high frequency of the following: words that are single characters, days of the week, and “says” or “said”. Days of the week and “says” or “said” are words commonly occurring in news articles. Hence, the pre-processing step is refined to include a custom list of stop words (each day of the week, “says”, “said”), and to only retain words that are longer than one character.</p>
<p>The code chunk below will perform the described pre-processing steps.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">## A function is defined to perform pre-processing steps for easier usage later on</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>custom_stop_word <span class="op">=</span> [<span class="st">'monday'</span>, <span class="st">'tuesday'</span>, <span class="st">'wednesday'</span>, <span class="st">'thursday'</span>, <span class="st">'friday'</span>, <span class="st">'saturday'</span>, <span class="st">'sunday'</span>, <span class="st">'said'</span>, <span class="st">'says'</span>, <span class="st">'s'</span>, <span class="st">'t'</span>]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>stop_words <span class="op">=</span> STOPWORDS.union(<span class="bu">set</span>(custom_stop_word)) <span class="co"># setting the stopwords</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>stemmer <span class="op">=</span> PorterStemmer() <span class="co"># setting the stemmer</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess_tokens(text):</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> re.sub(<span class="st">'Reuters'</span>, <span class="st">''</span>, text) <span class="co"># removing the word Reuters as it appears in all the real news</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> re.sub(<span class="st">'https?:\S+|www\.\S+'</span>, <span class="st">' '</span>, words) <span class="co"># remove URLs that begin with 'https' or 'www'</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> re.sub(<span class="st">'bit\.ly\S+'</span>, <span class="st">' '</span>, words) <span class="co"># remove URLS that begin with 'bit.ly' - only appears in true news</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> re.sub(<span class="st">'pic\.twitter\.com\S+'</span>, <span class="st">' '</span>, words) <span class="co"># remove URLs that begin with 'pic.twitter.com' - only appears in fake news</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> word_tokenize(words.lower())  <span class="co"># convert text to lowercase &amp; split into word tokens</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> <span class="kw">not</span> word <span class="kw">in</span> stop_words] <span class="co"># removing the stop words</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> [stemmer.stem(word) <span class="cf">for</span> word <span class="kw">in</span> words] <span class="co"># stemming the words</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> word.isalpha()] <span class="co"># removing punctuation</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> <span class="bu">len</span>(word) <span class="op">&gt;</span> <span class="dv">1</span>] <span class="co"># remove single character words</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> words</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The preprrocess_token() function can now be applied onto the text column of our dataframe.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'text_preprocessed'</span>] <span class="op">=</span> df[<span class="st">"text"</span>].<span class="bu">apply</span>(preprocess_tokens)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">text_preprocessed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>BREAKING: GOP Chairman Grassley Has Had Enoug...</td>
<td>1</td>
<td>[break, gop, chairman, grassley, demand, trump...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Failed GOP Candidates Remembered In Hilarious...</td>
<td>1</td>
<td>[fail, gop, candid, rememb, hilari, mock, eulo...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Mike Pence’s New DC Neighbors Are HILARIOUSLY...</td>
<td>1</td>
<td>[mike, penc, new, dc, neighbor, hilari, troll,...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>California AG pledges to defend birth control ...</td>
<td>0</td>
<td>[california, ag, pledg, defend, birth, control...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>AZ RANCHERS Living On US-Mexico Border Destroy...</td>
<td>1</td>
<td>[az, rancher, live, border, destroy, nanci, pe...</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Let’s determine the distribution of lengths of each token using a plot.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the length of each list of preprocessed tokens</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>lengths <span class="op">=</span> [<span class="bu">len</span>(tokens) <span class="cf">for</span> tokens <span class="kw">in</span> df[<span class="st">'text_preprocessed'</span>]]</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the distribution of the length of the list of preprocessed tokens</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>sns.histplot(lengths, bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">'skyblue'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, kde<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Distribution of the Length of Preprocessed Tokens'</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Length of Preprocessed Tokens'</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="fake_news_files/figure-html/cell-15-output-1.png" width="685" height="523"></p>
</div>
</div>
<p>Alternatively, it is also possible to determine the top 20 most common tokens in the entire corpus. Due to the large number of articles in the corpus, we can sample just the first 1,000 articles in order to get a glimpse of the top 20 most common tokens.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>first1000 <span class="op">=</span> <span class="bu">list</span>(itertools.chain.from_iterable(df[<span class="st">'text_preprocessed'</span>][:<span class="dv">1000</span>]))</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'most frequent tokens in first 1000 articles </span><span class="ch">\n</span><span class="st">'</span>, nltk.FreqDist(first1000).most_common(<span class="dv">20</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>most frequent tokens in first 1000 articles 
 [('trump', 3216), ('presid', 1328), ('state', 1272), ('peopl', 891), ('republican', 808), ('new', 693), ('obama', 692), ('year', 663), ('democrat', 651), ('like', 643), ('hous', 641), ('donald', 627), ('elect', 611), ('white', 610), ('support', 597), ('nation', 583), ('clinton', 572), ('parti', 572), ('report', 563), ('vote', 559)]</code></pre>
</div>
</div>
<p>We can see that topics related to United States (U.S) politics dominate the top tokens. These include tokens related to the U.S presidents such as Donald Trump or Barack Obama.</p>
</section>
<section id="document-classification---baseline" class="level2">
<h2 class="anchored" data-anchor-id="document-classification---baseline">Document Classification - Baseline</h2>
<p>As the project aims to understand the impact of using sentiment analysis to improve the accuracy of fake news classification using machine learning techniques, a baseline classification model is trained using only the processed data that excludes additional features.</p>
<p>Two model evaluation methods are explored in this project. The Train-Test Split Method is used and validated using the evaluation metrics, the K-Fold Cross Validation Method is used. As mentioned in the previous section, three classification models are employed in this project: XGBoost, SVM, and Logistic Regression.</p>
<p>First, the processed data is split into 70% for training and 30% for testing. This ratio is decided as it provides sufficient data to train the classification models (31,429 records), and adequate data to evaluate the performance of the models (13,469 records). Furthermore, this split also balances the computational resources required to train the models and leaves enough data for K-Fold Cross Validation.</p>
<p>Next, the processed data is vectorized using the TfidfVectorizer, which converts raw strings into a matrix of Term Frequency-Inverse Document Frequency (TF-IDF) features. This allows a higher weightage to be assigned to words that occur less frequently in the corpus as these words are more discriminative. The split X_train data is vectorized using the .fit_transform() function while the X_test data is vectorized using the .transform() function.</p>
<p>Next, we define a function to return the evaluation results for each model. The function makes predictions using the test data and calculates the evaluation metrics. The metrics are calculated using the classification_report() function from sklearn and it includes the precision, recall, f1-score, and support. The function also generates the confusion matrix created using the confusion_matrix() function from sklearn and the AUC-ROC curve, which plots true positive rate against false positive rate. This function can be called with each model to evaluate them separately.</p>
<p>Using the train-test split method alone may introduce bias that is an over- or underestimation of the performance of a model. K-fold cross-validation can be used to conduct a systematic evaluation of the model (Kohavi, 1995).</p>
<p>To achieve this, we wrote another function to evaluate each model using the k-fold evaluation method, where k = 10 in this project. This is implemented using the StratifiedKFold function from sklearn. This function splits the entire vectorized data set into 10 equal-sized parts with approximately similar proportions of class 0 and class 1 data. Next, we evaluated each model 10 times, each time using one fold as the validation set, and the average of the performance over 10 rounds of training and validation is used as the overall performance metric. This is achieved using the cross_validate() function from sklearn, and the precision, recall, f1-score, and support scores are created using the make_scorer() function from sklearn.</p>
<p>After obtaining the results of the baseline model, the data frame is fitted with features from the subsequent analyses – sentiment analysis using VADER and TextBlob. The train-test split method and k-fold cross-validation are applied to the corpus with the addition of the following features:</p>
<ul>
<li><p>Compound score from VADER</p></li>
<li><p>Polarity and Subjectivity scores from TextBlob</p></li>
<li><p>Compound score from VADER, and Polarity and Subjectivity scores from TextBlob</p></li>
</ul>
<p>But first, let’s consider the performance of a simple baseline solution without any additional features. This can be performed using the code chunks below.</p>
<section id="simple-train---test-model-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="simple-train---test-model-evaluation">Simple Train - Test Model Evaluation</h3>
<p>In this section, our baseline solution will only split the data into a train and test set in order to evaluate the performance of each model.</p>
<p>First, we define variables to store the independent variable, or the text, and the dependent variable, or the class of Fake or True.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[<span class="st">'text_preprocessed'</span>]</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'class'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we split the data into a train and test set in order to make sure the models can be trained and evaluated on separate data sets, or in other words, to make sure that each model is not being tested on data it has seen during training.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Because the TfidfVectorizer will not work with word tokens, our tokens would need to be passed through a dummy function first. Essentially, the TfidfVectorizer has its own tokenizer argument meant to tokenize input strings. By inputing our dummy function as a tokenizer, we can use the tokens we have already created.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dummy(tokens):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokens</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer(tokenizer<span class="op">=</span>dummy, </span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>                             preprocessor<span class="op">=</span>dummy,</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>                             token_pattern<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>                             ngram_range <span class="op">=</span> (<span class="dv">1</span>,<span class="dv">1</span>), </span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>                             min_df <span class="op">=</span> <span class="fl">0.1</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> vectorizer.fit_transform(X_train)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> vectorizer.transform(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we can create a dictionary of models we would like to evaluate. For this analysis, we will use three models discussed during the course: XGBoost, Support Vector Machine (SVM), and Logistic Regression.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> {</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'XGBoost'</span>: xgb.XGBClassifier(),</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SVM'</span>: CalibratedClassifierCV(svm.LinearSVC()),</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'LogReg'</span>: LogisticRegression(class_weight<span class="op">=</span><span class="st">'balanced'</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A function called model_evaluate is defined in order to evaluate each model and return evaluations in terms of the visualization of a confusion matrix and classification report of different metrics (F1, accuracy, precision, recall). Additionally, an ROC curve is drawn to visually evaluate the performance of the models.</p>
<p>model_evaluate will take each model from the pre-defined dictionary, fit the training data in X_test to it, and then evaluate its performance on the test corpus in X_test.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model_evaluate(models, model_name):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the pipeline</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    pipeline <span class="op">=</span> Pipeline(steps<span class="op">=</span>[</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        (<span class="st">'model'</span>, models[model_name])</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit the pipeline on the training data</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    pipeline.fit(X_train, y_train)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make predictions on the test data</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> pipeline.predict(X_test)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate evaluation metrics</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Summary report for </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(model_name))</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Classification Report:"</span>)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_test, y_pred))</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Confusion Matrix</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Confusion Matrix:"</span>)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    cm <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    cmplot <span class="op">=</span> ConfusionMatrixDisplay(cm)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    cmplot.plot()</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># AUC-ROC Curve</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"AUC-ROC Curve:"</span>)</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>    y_pred_proba <span class="op">=</span> pipeline.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    auc_roc <span class="op">=</span> roc_auc_score(y_test, y_pred_proba)</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>    fpr, tpr, _ <span class="op">=</span> roc_curve(y_test, y_pred_proba)</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the ROC curve</span></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    plt.plot(fpr, tpr, label<span class="op">=</span><span class="st">'ROC curve (area = </span><span class="sc">%0.4f</span><span class="st">)'</span> <span class="op">%</span> auc_roc)</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>    plt.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], <span class="st">'k--'</span>)</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>    plt.xlim([<span class="fl">0.0</span>, <span class="fl">1.0</span>])</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>    plt.ylim([<span class="fl">0.0</span>, <span class="fl">1.05</span>])</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'False Positive Rate'</span>)</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'True Positive Rate'</span>)</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Receiver Operating Characteristic'</span>)</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>    plt.legend(loc<span class="op">=</span><span class="st">"lower right"</span>)</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The evaluation of the XGBoost model:</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>model_evaluate(models,<span class="st">'XGBoost'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Summary report for XGBoost
Classification Report:
              precision    recall  f1-score   support

           0       0.96      0.96      0.96      6448
           1       0.96      0.96      0.96      7022

    accuracy                           0.96     13470
   macro avg       0.96      0.96      0.96     13470
weighted avg       0.96      0.96      0.96     13470

Confusion Matrix:
AUC-ROC Curve:</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="fake_news_files/figure-html/cell-22-output-2.png" width="513" height="429"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="fake_news_files/figure-html/cell-22-output-3.png" width="674" height="523"></p>
</div>
</div>
<p>The evaluation of the SVM model:</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>model_evaluate(models,<span class="st">'SVM'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Summary report for SVM
Classification Report:
              precision    recall  f1-score   support

           0       0.91      0.93      0.92      6448
           1       0.94      0.91      0.93      7022

    accuracy                           0.92     13470
   macro avg       0.92      0.92      0.92     13470
weighted avg       0.92      0.92      0.92     13470

Confusion Matrix:
AUC-ROC Curve:</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="fake_news_files/figure-html/cell-23-output-2.png" width="513" height="429"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="fake_news_files/figure-html/cell-23-output-3.png" width="674" height="523"></p>
</div>
</div>
<p>The evaluation of the Logistic Regression model:</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>model_evaluate(models,<span class="st">'LogReg'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Summary report for LogReg
Classification Report:
              precision    recall  f1-score   support

           0       0.90      0.94      0.92      6448
           1       0.94      0.91      0.92      7022

    accuracy                           0.92     13470
   macro avg       0.92      0.92      0.92     13470
weighted avg       0.92      0.92      0.92     13470

Confusion Matrix:
AUC-ROC Curve:</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="fake_news_files/figure-html/cell-24-output-2.png" width="513" height="429"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="fake_news_files/figure-html/cell-24-output-3.png" width="674" height="523"></p>
</div>
</div>
<p>Based on the evaluation of the three models, the XGBoost model can be considered to be the most effective as it achieved the highest F1-score and the highest accuracy of 96%. In order to improve the rigor of our machine learning, K-fold Cross Validation can be performed to further evaluate each of the model.</p>
</section>
<section id="k-fold-cross-validation-method" class="level3">
<h3 class="anchored" data-anchor-id="k-fold-cross-validation-method">K-fold Cross Validation Method</h3>
<p>It is possible to tweak our functions to perform StratifiedKFold validation instead of a simple train-test split.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>scoring <span class="op">=</span> {<span class="st">'accuracy'</span> : make_scorer(accuracy_score), </span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>           <span class="st">'precision'</span> : make_scorer(precision_score),</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>           <span class="st">'recall'</span> : make_scorer(recall_score), </span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>           <span class="st">'f1_score'</span> : make_scorer(f1_score)}</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>           </span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model_evaluate_kfold(models, model_name):</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    pipeline <span class="op">=</span> Pipeline(steps<span class="op">=</span>[</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        (<span class="st">'vectorizer'</span>, TfidfVectorizer(tokenizer<span class="op">=</span>dummy, preprocessor<span class="op">=</span>dummy, token_pattern<span class="op">=</span><span class="va">None</span>, ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), min_df<span class="op">=</span><span class="fl">0.1</span>)),</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        (<span class="st">'model'</span>, models[model_name])</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use stratified k-fold cross-validation</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform cross-validation</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> cross_validate(pipeline, X, y, cv<span class="op">=</span>cv, scoring<span class="op">=</span>scoring)</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Cross-validation scores for </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>scores[<span class="st">'test_accuracy'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span>scores[<span class="st">'test_precision'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Recall: </span><span class="sc">{</span>scores[<span class="st">'test_recall'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"F1-score: </span><span class="sc">{</span>scores[<span class="st">'test_f1_score'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Mean accuracy: </span><span class="sc">{</span>scores[<span class="st">'test_accuracy'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Standard deviation: </span><span class="sc">{</span>scores[<span class="st">'test_accuracy'</span>]<span class="sc">.</span>std()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, it is possible to evaluate our models, starting with XGBoost. Due to the nature of k-fold validation, the process would take longer than simple train-test evaluation:</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>model_evaluate_kfold(models,<span class="st">'XGBoost'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cross-validation scores for XGBoost:
Accuracy: [0.96592428 0.96436526 0.96013363 0.96035635 0.96124722 0.96792873
 0.96681514 0.9701559  0.96435732 0.96435732]
Precision: 0.9645687925543933
Recall: 0.9678037629554204
F1-score: 0.9661795229357942
Mean accuracy: 0.9645641139117099
Standard deviation: 0.0031299726702493265</code></pre>
</div>
</div>
<p>It is possible to see that the Accuracy and F1-score improved by almost one percentage point when k-fold validation is performed instead of a simple train-test training.</p>
<p>Let’s continue to evaluate the SVM and Logistic Regression models.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>model_evaluate_kfold(models,<span class="st">'SVM'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cross-validation scores for SVM:
Accuracy: [0.9247216  0.92249443 0.92672606 0.91737194 0.9247216  0.93006682
 0.92227171 0.922049   0.92537313 0.91713076]
Precision: 0.9358082510351204
Recall: 0.916187521893038
F1-score: 0.9258872932004186
Mean accuracy: 0.9232927061001874
Standard deviation: 0.0037727925841135918</code></pre>
</div>
</div>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>model_evaluate_kfold(models,<span class="st">'LogReg'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cross-validation scores for LogReg:
Accuracy: [0.922049   0.92227171 0.92583519 0.91915367 0.92405345 0.92806236
 0.91937639 0.91982183 0.92269993 0.91735353]
Precision: 0.9404459114087015
Recall: 0.9085218038340285
F1-score: 0.9242044200936856
Mean accuracy: 0.9220677072040985
Standard deviation: 0.0031227689149698145</code></pre>
</div>
</div>
<p>It is possible to detect a slight increase in each evaluation metric for the three models. Moving forward, it can be determined that XGBoost and k-fold cross validation can be used for further experimentation. The baseline results using XGBoost can be stored for later comparison.</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>pipeline <span class="op">=</span> Pipeline(steps<span class="op">=</span>[</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'vectorizer'</span>, TfidfVectorizer(tokenizer<span class="op">=</span>dummy, preprocessor<span class="op">=</span>dummy, token_pattern<span class="op">=</span><span class="va">None</span>, ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), min_df<span class="op">=</span><span class="fl">0.1</span>)),</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'model'</span>, models[<span class="st">'XGBoost'</span>])</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Use stratified k-fold cross-validation</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform cross-validation</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> cross_validate(pipeline, X, y, cv<span class="op">=</span>cv, scoring<span class="op">=</span>scoring)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>scores_baseline <span class="op">=</span> [scores[<span class="st">'test_f1_score'</span>].mean(), scores[<span class="st">'test_accuracy'</span>].mean()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Following the determination of a model and evaluation method, Sentiment Analysis can be performed in order to understand the sentiments of fake news versus real news and to potentially create new features for fake news detection with machine learning.</p>
</section>
</section>
<section id="sentiment-analysis" class="level2">
<h2 class="anchored" data-anchor-id="sentiment-analysis">Sentiment Analysis</h2>
<p>Sentiment analysis, or opinion mining, aims to identify the emotional pattern within written text. By conducting sentiment analysis, the sentiment, attitudes, and emotions expressed by a piece of text can be determined (Liu, 2020). Such analysis is particularly relevant to the fake news detection as targeting emotions and provoking an emotional response are the main vectors of effectiveness and spread for fraudulent information (Bakir &amp; McStay, 2018; Horner, et al., 2021). In addition, due to the prominent role that sentiments play in fake news, multiple studies have found that by including sentiment analysis, the performance of a fake news classifier can be improved (Ajao et al., 2019; Alonso et al., 2021; Zhang et al., 2021).</p>
<p>In this project, the goal of sentiment analysis is two-fold. First, sentiment analysis is an objective in and of itself, allowing the study to capture any emotional disparity between fake and genuine news and describe the usage of emotions in fake news. Second, the results of sentiment analysis can serve as new features for document classification, potentially improving its performance. We utilized two of the most popular Python packages, NLTK and TextBlob. However, it is important to note that a clear drawback of any sentiment analysis task based on existing dictionaries will be that the context of the dictionary might not match with the context of the current corpus.</p>
<p>For the purpose of this project, sentiment analysis will be performed using two libraries, NLTK’s VADER and TextBlob.</p>
<section id="vader" class="level3">
<h3 class="anchored" data-anchor-id="vader">VADER</h3>
<p>NLTK is a powerful tool for natural language processing, including classifiers and sentiment lexicons for sentiment analysis. It works with many text analytics tasks, provides users with a pretrained lexicon and rule-based sentiment analyzer called Valence Aware Dictionary and sEntiment Reasoner (VADER) (Mogyorosi, n.d.). The VADER analyzer uses the vader_lexicon developed by C.J. Hutto and Eric Gilbert, which measures a sentiment intensity score from words, phrases and emojis, and based on their emotional content, from negative to positive. This vader_lexicon can be applied to a piece of written text using NLTK’s SentimentIntensityAnalyzer’s polarity_score() function. SentimentIntensityAnalyzer can be applied to whole sentences instead of tokenized text. However, we note that VADER was trained specifically for social media data to handle complex language sentiments, which may not be completely applicable to fake news.</p>
<p>As the VADER lexicon can be applied directly to text using the SentimentIntensityAnalyzer(), polarity scores for each document in the corpus can be generated by applying .polarity_score() on each record of the processed text dataframe, df. Four score types can be generated from .polarity_score(), and we store each score in one newly created column in df:</p>
<ul>
<li><p>Compound: overall sentiment score between -1 to 1</p></li>
<li><p>Negative: negative sentiment score between 0 to 1</p></li>
<li><p>Neutral: neutral sentiment score between 0 to 1</p></li>
<li><p>Positive: positive sentiment score between 0 to 1</p></li>
</ul>
<p>The compound score is used as a feature in the classification model as it represents the overall sentiment of each document. The compound score is the sum of the valence scores of each word in the lexicon, normalized to be between -1 and +1 (Swarnkar, 2020).</p>
<p>First, we load the SentimentIntensityAnalyzer into a variable.</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>sen_ana <span class="op">=</span> SentimentIntensityAnalyzer()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s test how the results of the analyzer are displayed.</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sen_ana.polarity_scores(<span class="st">'I love quiz'</span>))</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sen_ana.polarity_scores(<span class="st">'I hate quiz'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}
{'neg': 0.787, 'neu': 0.213, 'pos': 0.0, 'compound': -0.5719}</code></pre>
</div>
</div>
<p>We can see that the first statement is rated to be very positive while the second statement is considered very negative, which makes sense with a cursory inspection.</p>
<p>Instead of having all four scores combined as above, it would be easier for analysis if they’re stored in separate columns. To do so, we can assign each of the score to a new column in the data frame.</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'nltk_compound'</span>]  <span class="op">=</span> df[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: sen_ana.polarity_scores(x)[<span class="st">'compound'</span>])</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'nltk_neg'</span>] <span class="op">=</span> df[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: sen_ana.polarity_scores(x)[<span class="st">'neg'</span>])</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'nltk_neu'</span>] <span class="op">=</span> df[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: sen_ana.polarity_scores(x)[<span class="st">'neu'</span>])</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'nltk_pos'</span>] <span class="op">=</span> df[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: sen_ana.polarity_scores(x)[<span class="st">'pos'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It is now possible to see the average sentiment scores by class of articles.</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>df.groupby(<span class="st">'class'</span>)[[<span class="st">'nltk_neg'</span>,<span class="st">'nltk_neu'</span>,<span class="st">'nltk_pos'</span>,<span class="st">'nltk_compound'</span>]].mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">nltk_neg</th>
<th data-quarto-table-cell-role="th">nltk_neu</th>
<th data-quarto-table-cell-role="th">nltk_pos</th>
<th data-quarto-table-cell-role="th">nltk_compound</th>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0.078701</td>
<td>0.839465</td>
<td>0.081833</td>
<td>0.055346</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0.101657</td>
<td>0.805650</td>
<td>0.092697</td>
<td>-0.109791</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>It can be observed that the sentiment differences between the two classes of fake and true news are not significantly divergent based on their mean scores. A visualization might help to further elucidate any potential differences.</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>sns.histplot(df, x<span class="op">=</span><span class="st">"nltk_neg"</span>, hue<span class="op">=</span><span class="st">"class"</span>, stat<span class="op">=</span><span class="st">"count"</span>,ax<span class="op">=</span>axs[<span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>sns.histplot(df, x<span class="op">=</span><span class="st">"nltk_neu"</span>, hue<span class="op">=</span><span class="st">"class"</span>, stat<span class="op">=</span><span class="st">"count"</span>,ax<span class="op">=</span>axs[<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>sns.histplot(df, x<span class="op">=</span><span class="st">"nltk_pos"</span>, hue<span class="op">=</span><span class="st">"class"</span>, stat<span class="op">=</span><span class="st">"count"</span>,ax<span class="op">=</span>axs[<span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>sns.histplot(df, x<span class="op">=</span><span class="st">"nltk_compound"</span>, hue<span class="op">=</span><span class="st">"class"</span>, stat<span class="op">=</span><span class="st">"count"</span>,ax<span class="op">=</span>axs[<span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="fake_news_files/figure-html/cell-34-output-1.png" width="1141" height="756"></p>
</div>
</div>
</section>
<section id="textblob" class="level3">
<h3 class="anchored" data-anchor-id="textblob">TextBlob</h3>
<p>TextBlob is a user-friendly library built on NLTK library, allowing users to easily analyze a text corpus. It is more oriented towards user reviews and based on the “pattern” library, which contains high frequency adjectives in reviews. In addition to a customizable library of adjectives, TextBlob also provides a library created through machine learning, namely a Naïve Bayes Classifier model to classify text as positive, negative, or neutral. This library applied a machine-learning model on a corpus of movie reviews and score rating to derive the ratings of words (Loria, 2020).</p>
<p>The TextBlob().sentiment function is applied directly to each record of the processed text dataframe, df. Two score types are generated from this function, and each is stored in one newly created column in df:</p>
<ul>
<li><p>Polarity: measures negative or positive sentiment between -1 and 1, with -1 defined as a negative sentiment and 1 defined as a positive sentiment.</p></li>
<li><p>Subjectivity: quantifies the amount of personal opinion and factual information contained in the text between 0 and 1. Higher subjectivity means that the text contains personal opinion than facts</p></li>
</ul>
<p>Let’s see a demonstration of TextBlob in Python.</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [<span class="st">'I love quiz'</span>, <span class="st">'I hate quiz'</span>]</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create TextBlob objects with the text</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Analyze sentiments and print</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> texts:</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    sentiment <span class="op">=</span> TextBlob(text).sentiment</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Sentiment for '</span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>sentiment<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Sentiment for 'I love quiz': Sentiment(polarity=0.5, subjectivity=0.6)
Sentiment for 'I hate quiz': Sentiment(polarity=-0.8, subjectivity=0.9)</code></pre>
</div>
</div>
<p>Similar to VADER, TextBlob identified the first statement as having a positive polarity score while the second as having a negative polarity score. It is important to note that even though a cursory examination shows both statements to be equally subjective, TextBlob indicates that one is more subjective than the other.</p>
<p>TextBlob can be applied to our corpus and the resulting scores can be stored in their own respective columns.</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'textblob_sentiment'</span>] <span class="op">=</span> df[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: TextBlob(x).sentiment)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'textblob_polarity'</span>] <span class="op">=</span> df[<span class="st">'textblob_sentiment'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.polarity)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'textblob_subjectivity'</span>] <span class="op">=</span> df[<span class="st">'textblob_sentiment'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.subjectivity)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, the mean polarity and subjectivity scores from TextBlob can be calculated.</p>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>sentiment_means <span class="op">=</span> df.groupby(<span class="st">'class'</span>)[[<span class="st">'textblob_polarity'</span>, <span class="st">'textblob_subjectivity'</span>]].mean()</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sentiment_means)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       textblob_polarity  textblob_subjectivity
class                                          
0               0.053636               0.360700
1               0.056655               0.452103</code></pre>
</div>
</div>
<p>Once again, the difference between the Fake and True news classes do not seem to be significant. It might be useful to visualize the distribution of scores in order to confirm this observation.</p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>sns.histplot(df, x<span class="op">=</span><span class="st">"textblob_polarity"</span>, hue<span class="op">=</span><span class="st">"class"</span>, stat<span class="op">=</span><span class="st">"count"</span>, ax <span class="op">=</span> axs[<span class="dv">0</span>])</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>sns.histplot(df, x<span class="op">=</span><span class="st">"textblob_subjectivity"</span>, hue<span class="op">=</span><span class="st">"class"</span>, stat<span class="op">=</span><span class="st">"count"</span>, ax <span class="op">=</span> axs[<span class="dv">1</span>])</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="fake_news_files/figure-html/cell-39-output-1.png" width="1141" height="756"></p>
</div>
</div>
<p>Let’s combine all the scores into one table for easier comparison.</p>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>nltk_sentiment <span class="op">=</span> df.groupby(<span class="st">'class'</span>)[[<span class="st">'nltk_neg'</span>,<span class="st">'nltk_neu'</span>,<span class="st">'nltk_pos'</span>,<span class="st">'nltk_compound'</span>]].mean()</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>textblob_sentiment <span class="op">=</span> df.groupby(<span class="st">'class'</span>)[[<span class="st">'textblob_polarity'</span>, <span class="st">'textblob_subjectivity'</span>]].mean()</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>summary_table <span class="op">=</span> pd.concat([nltk_sentiment,textblob_sentiment],axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>summary_table.index <span class="op">=</span> [<span class="st">'True'</span>,<span class="st">'Fake'</span>]</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>summary_table</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">nltk_neg</th>
<th data-quarto-table-cell-role="th">nltk_neu</th>
<th data-quarto-table-cell-role="th">nltk_pos</th>
<th data-quarto-table-cell-role="th">nltk_compound</th>
<th data-quarto-table-cell-role="th">textblob_polarity</th>
<th data-quarto-table-cell-role="th">textblob_subjectivity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">True</td>
<td>0.078701</td>
<td>0.839465</td>
<td>0.081833</td>
<td>0.055346</td>
<td>0.053636</td>
<td>0.360700</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Fake</td>
<td>0.101657</td>
<td>0.805650</td>
<td>0.092697</td>
<td>-0.109791</td>
<td>0.056655</td>
<td>0.452103</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>It can be seen that Fake News have an NLTK Negative mean score closer to –1 (0.101657), indicating that Fake News articles contain more words linked to negative emotions than real true news. However, if TextBlob scores are considered, the Polarity score for True News is slightly smaller, indicating that True News is more negative. Yet, these scores are so similar that it might not indicate a meaningful difference. At the same time, it is possible that Fake News, in general, contain more emotions, either negative or positive, compared to True News. This is reflected in Fake News higher score for NLTK Positive Sentiment (0.092697) compared to True News (0.081833). By encompassing both more negative and positive emotions, the mean TextBlob Polarity score for Fake News can be normalized.</p>
<p>On the other hand, the Subjectivity scores exhibit a larger difference between Fake (0.452103) and True News (0.3607), reflecting the potential for Fake News to contain more opinions than facts compared to True News. Histograms visualizing the distribution difference between True and Fake news can be referenced in the Appendix.</p>
<p>Because a difference seems to exist between True News and Fake News for different sentiment categories, the idea that sentiment scores can be added as a feature for the classification task is further reinforced.</p>
<p>In the following section, the document classification model using XGBoost and k-fold cross validation will be refitted to a set of features including the vectorized text of the corpus as well as the compound sentiment scores from NLTK (Compound Score) and TextBlob (Polarity Score) and subjectivity score from TextBlob (Subjectivity Score). Each of these feature will be added sequentially, then combined together to discern their effect on model performance.</p>
</section>
</section>
<section id="document-classification-with-sentiment-analysis" class="level2">
<h2 class="anchored" data-anchor-id="document-classification-with-sentiment-analysis">Document Classification with Sentiment Analysis</h2>
<section id="document-classification-with-sentiment-analysis-nltk-vader" class="level3">
<h3 class="anchored" data-anchor-id="document-classification-with-sentiment-analysis-nltk-vader">Document Classification with Sentiment Analysis (NLTK VADER)</h3>
<p>Firstly, we can perform document classification using NLTK’s compound score with XGBoost and k-fold cross validation.</p>
<p>Only one step is added to the process, which is to use hstack from numpy in order to stack the features column wise after vectorization of text features.</p>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the dependent and independent variables</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> df[<span class="st">'text_preprocessed'</span>]</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>x_sentiment_compound <span class="op">=</span> df[<span class="st">'nltk_compound'</span>].values.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'class'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create features with compound sentiment score</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dummy(tokens):</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokens</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer(tokenizer<span class="op">=</span>dummy, </span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>                             preprocessor<span class="op">=</span>dummy, </span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>                             token_pattern<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>                             ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>), </span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>                             min_df<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>                             </span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>x_feature <span class="op">=</span> vectorizer.fit_transform(x)</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>x_compound <span class="op">=</span> hstack((x_feature,x_sentiment_compound))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>scoring <span class="op">=</span> {<span class="st">'accuracy'</span> : make_scorer(accuracy_score), </span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>           <span class="st">'precision'</span> : make_scorer(precision_score),</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>           <span class="st">'recall'</span> : make_scorer(recall_score), </span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>           <span class="st">'f1_score'</span> : make_scorer(f1_score)}</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> xgb.XGBClassifier()</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Use stratified k-fold cross-validation</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform cross-validation</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> cross_validate(model, x_compound, y, cv<span class="op">=</span>cv, scoring<span class="op">=</span>scoring)</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>scores_nltk <span class="op">=</span> [scores[<span class="st">'test_f1_score'</span>].mean(),scores[<span class="st">'test_accuracy'</span>].mean()]</span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cross-validation scores for XGBoost:"</span>)</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>scores[<span class="st">'test_accuracy'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span>scores[<span class="st">'test_precision'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Recall: </span><span class="sc">{</span>scores[<span class="st">'test_recall'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1-score: </span><span class="sc">{</span>scores[<span class="st">'test_f1_score'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean accuracy: </span><span class="sc">{</span>scores[<span class="st">'test_accuracy'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Standard deviation: </span><span class="sc">{</span>scores[<span class="st">'test_accuracy'</span>]<span class="sc">.</span>std()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cross-validation scores for XGBoost:
Accuracy: [0.96525612 0.96659243 0.96481069 0.95924276 0.96458797 0.97238307
 0.96859688 0.97238307 0.96680775 0.96658499]
Precision: 0.9668479660630778
Recall: 0.9696350362581343
F1-score: 0.9682334814359199
Mean accuracy: 0.9667245744485035
Standard deviation: 0.0036598976506881397</code></pre>
</div>
</div>
<p>It can be observed that the F1 score and mean accuracy both demonstrate a marginal increase of around 0.01%, showing that adding features related to sentiment analysis could possibly make a difference. The next section will explore the use of TextBlob’s sentiment and subjectivity scores as features and the combined sets of NLTK and TextBlob features.</p>
</section>
<section id="document-classification-with-sentiment-analysis-textblob" class="level3">
<h3 class="anchored" data-anchor-id="document-classification-with-sentiment-analysis-textblob">Document Classification with Sentiment Analysis (TextBlob)</h3>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the dependent and independent variables</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> df[<span class="st">'text_preprocessed'</span>]</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>x_sentiment_polarity <span class="op">=</span> df[<span class="st">'textblob_polarity'</span>].values.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>x_sentiment_subjectivity <span class="op">=</span> df[<span class="st">'textblob_subjectivity'</span>].values.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'class'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create features with polarity and subjectivity sentiment score</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dummy(tokens):</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokens</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer(tokenizer<span class="op">=</span>dummy, preprocessor<span class="op">=</span>dummy, token_pattern<span class="op">=</span><span class="va">None</span>, ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>), min_df<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>x_feature <span class="op">=</span> vectorizer.fit_transform(x)</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>x_pol_sub <span class="op">=</span> hstack((x_feature, x_sentiment_polarity, x_sentiment_subjectivity))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>scoring <span class="op">=</span> {<span class="st">'accuracy'</span> : make_scorer(accuracy_score), </span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>           <span class="st">'precision'</span> : make_scorer(precision_score),</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>           <span class="st">'recall'</span> : make_scorer(recall_score), </span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>           <span class="st">'f1_score'</span> : make_scorer(f1_score)}</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> xgb.XGBClassifier()</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Use stratified k-fold cross-validation</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform cross-validation</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> cross_validate(model, x_pol_sub, y, cv<span class="op">=</span>cv, scoring<span class="op">=</span>scoring)</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>scores_textblob <span class="op">=</span> [scores[<span class="st">'test_f1_score'</span>].mean(),scores[<span class="st">'test_accuracy'</span>].mean()]</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cross-validation scores for XGBoost:"</span>)</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>scores[<span class="st">'test_accuracy'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span>scores[<span class="st">'test_precision'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Recall: </span><span class="sc">{</span>scores[<span class="st">'test_recall'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1-score: </span><span class="sc">{</span>scores[<span class="st">'test_f1_score'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean accuracy: </span><span class="sc">{</span>scores[<span class="st">'test_accuracy'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Standard deviation: </span><span class="sc">{</span>scores[<span class="st">'test_accuracy'</span>]<span class="sc">.</span>std()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cross-validation scores for XGBoost:
Accuracy: [0.9674833  0.97060134 0.96035635 0.96213808 0.96592428 0.9714922
 0.96815145 0.97349666 0.97037202 0.96858989]
Precision: 0.9672716036051199
Recall: 0.9714239558244728
F1-score: 0.9693375411244309
Mean accuracy: 0.9678605559444741
Standard deviation: 0.003903097924174063</code></pre>
</div>
</div>
</section>
<section id="document-classification-with-sentiment-analysis-nltk-vader-and-textblob" class="level3">
<h3 class="anchored" data-anchor-id="document-classification-with-sentiment-analysis-nltk-vader-and-textblob">Document Classification with Sentiment Analysis (NLTK VADER and TextBlob)</h3>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the dependent and independent variables</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> df[<span class="st">'text_preprocessed'</span>]</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>x_sentiment_compound <span class="op">=</span> df[<span class="st">'nltk_compound'</span>].values.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>x_sentiment_polarity <span class="op">=</span> df[<span class="st">'textblob_polarity'</span>].values.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>x_sentiment_subjectivity <span class="op">=</span> df[<span class="st">'textblob_subjectivity'</span>].values.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'class'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create features with compound, polarity, subjecctivity scores</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dummy(tokens):</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokens</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer(tokenizer<span class="op">=</span>dummy, preprocessor<span class="op">=</span>dummy, token_pattern<span class="op">=</span><span class="va">None</span>, ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>), min_df<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>x_feature <span class="op">=</span> vectorizer.fit_transform(x)</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>x_combined <span class="op">=</span> hstack((x_feature, x_sentiment_compound, x_sentiment_polarity, x_sentiment_subjectivity))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>scoring <span class="op">=</span> {<span class="st">'accuracy'</span> : make_scorer(accuracy_score), </span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>           <span class="st">'precision'</span> : make_scorer(precision_score),</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>           <span class="st">'recall'</span> : make_scorer(recall_score), </span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>           <span class="st">'f1_score'</span> : make_scorer(f1_score)}</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> xgb.XGBClassifier()</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Use stratified k-fold cross-validation</span></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform cross-validation</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> cross_validate(model, x_combined, y, cv<span class="op">=</span>cv, scoring<span class="op">=</span>scoring)</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>scores_nltk_textblob <span class="op">=</span> [scores[<span class="st">'test_f1_score'</span>].mean(),scores[<span class="st">'test_accuracy'</span>].mean()]</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cross-validation scores for XGBoost:"</span>)</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>scores[<span class="st">'test_accuracy'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span>scores[<span class="st">'test_precision'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Recall: </span><span class="sc">{</span>scores[<span class="st">'test_recall'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1-score: </span><span class="sc">{</span>scores[<span class="st">'test_f1_score'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean accuracy: </span><span class="sc">{</span>scores[<span class="st">'test_accuracy'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Standard deviation: </span><span class="sc">{</span>scores[<span class="st">'test_accuracy'</span>]<span class="sc">.</span>std()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cross-validation scores for XGBoost:
Accuracy: [0.96859688 0.97037862 0.96503341 0.96280624 0.96636971 0.97060134
 0.96837416 0.97305122 0.97104032 0.96769882]
Precision: 0.9682607191411726
Recall: 0.9714238833009515
F1-score: 0.9698313664439555
Mean accuracy: 0.9683950721412053
Standard deviation: 0.0029093480197024363</code></pre>
</div>
</div>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([scores_baseline, scores_nltk,scores_textblob,scores_nltk_textblob], columns <span class="op">=</span> [<span class="st">'F1-score'</span>,<span class="st">'Accuracy'</span>], index <span class="op">=</span> [<span class="st">'Baseline XGBoost'</span>,<span class="st">'NLTK VADER'</span>,<span class="st">'TextBlob'</span>,<span class="st">'Combined'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">F1-score</th>
<th data-quarto-table-cell-role="th">Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Baseline XGBoost</td>
<td>0.966180</td>
<td>0.964564</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">NLTK VADER</td>
<td>0.968233</td>
<td>0.966725</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">TextBlob</td>
<td>0.969338</td>
<td>0.967861</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Combined</td>
<td>0.969831</td>
<td>0.968395</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>All three models that incorporated sentiment analysis scores (NLTK VADER, TextBlob, and Combined) showed an improvement in model performance compared to the baseline XGBoost model. These findings correlate with the studies cited that sentiment analysis may be a useful feature for enhancing model performance.</p>
</section>
</section>
<section id="discussions-and-gap-analysis" class="level2">
<h2 class="anchored" data-anchor-id="discussions-and-gap-analysis">Discussions and Gap Analysis</h2>
<section id="inclusion-of-sentiment-analysis-in-document-classification" class="level3">
<h3 class="anchored" data-anchor-id="inclusion-of-sentiment-analysis-in-document-classification"><strong>Inclusion of Sentiment Analysis in Document Classification</strong></h3>
<p>The results of document classification using the results of sentiment analysis as additional features exhibit improvements over base classification models using only text as features. This shows that the consideration of sentiment analysis could be a fruitful endeavor in the identification of fake news. Additionally, it reinforces the idea that fake news tends to have more emotion expressors than regular news to incite an emotional response from readers. What is lacking, however, is a word-sentiment dictionary which might be more appropriate to the context of news articles. Such a dictionary might further improve the results of the classifications by allowing for better calculation of sentiment scores.</p>
</section>
<section id="named-entities" class="level3">
<h3 class="anchored" data-anchor-id="named-entities"><strong>Named Entities</strong></h3>
<p>In news articles, it is very common for named entities to be used that might not be recognized in dictionaries, and with the lack of recognition, also comes the lack of context surrounding the entities. If the model can recognize these entities, this would then further enhance how it differentiates real news from fake news as there is additional information for the model to consider.</p>
</section>
<section id="data-source" class="level3">
<h3 class="anchored" data-anchor-id="data-source"><strong>Data Source</strong></h3>
<p>The Kaggle dataset that was used seems to likely be from the United States. Most of the fake news were related to America’s politics and thus the model has been tuned to pick up keywords along this theme. This means that if we were to apply this model to datasets from around the world, the model might not be as applicable or accurate anymore as the topics identified will likely not be as relevant. For example, news about Trump’s inauguration might not be as big in Singapore, and thus if the model was used as-is to run on Singapore’s news, the results produced would thereby not be as accurate.</p>
</section>
<section id="technical-limitations" class="level3">
<h3 class="anchored" data-anchor-id="technical-limitations"><strong>Technical Limitations</strong></h3>
<p>As we were dealing with a large dataset and performing complex NLP tasks, we faced constraints such as computational resources and processing speed issues. Running the code sometimes took more than an hour, due to the nature of the computations that we were running. This was quite inefficient, as we needed to test multiple models and had several revisions of code, resulting in us having to spend time allowing the code to load.</p>
</section>
<section id="dynamic-nature-of-fake-news" class="level3">
<h3 class="anchored" data-anchor-id="dynamic-nature-of-fake-news"><strong>Dynamic Nature of Fake News</strong></h3>
<p>As technology evolves, so do the methods of dissemination of fake news. With ever-changing tactics to deceive the masses, static models that are not continually updated will not be able to adapt to any emerging trends. As such, if the model is not regularly refreshed, it will sooner or later become obsolete and inaccurate.</p>
</section>
</section>
<section id="future-work-and-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="future-work-and-conclusion">Future Work and Conclusion</h2>
<section id="named-entity-recognition-ner" class="level3">
<h3 class="anchored" data-anchor-id="named-entity-recognition-ner"><strong>Named Entity Recognition (NER)</strong></h3>
<p>Named Entity Recognition (NER) can extract important entities mentioned in news articles, such as the names of politicians, organizations, or locations. By analyzing the entities mentioned in both fake and real news, patterns may emerge that could indicate the credibility or authenticity of the article. Additionally, by analyzing the context surrounding named entities, NER can help detect instances of misinformation or propaganda. For example, if a fake news article mentions a well-known organization or individual in a misleading context, NER can help identify the discrepancy between the entity’s actual role or stance and how it is portrayed in the article.</p>
</section>
<section id="extension-of-data-source-scope" class="level3">
<h3 class="anchored" data-anchor-id="extension-of-data-source-scope"><strong>Extension of Data Source Scope</strong></h3>
<p>To tackle the second issue, a future extension of this project would then be to re-train the model based on local news. This would then help to include more local topics on top of the American news (as Singapore is so interlinked to the rest of the world, American news would not be completely irrelevant, so keeping this would not be completely useless). The accuracy and precision of the model would then be improved, and thereby we will be able to use this model for local news as well.</p>
</section>
<section id="regular-updating-of-dataset" class="level3">
<h3 class="anchored" data-anchor-id="regular-updating-of-dataset"><strong>Regular Updating of Dataset</strong></h3>
<p>Our model will need to be updated regularly with more recent articles to ensure that it can continue to stay relevant and continue to identify fake news accurately, even those disseminated with new methods of trickery.</p>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references"><strong>References</strong></h2>
<p>Ajao, O., Bhowmik, D., &amp; Zargari, S. (2019). Sentiment Aware Fake News Detection on Online Social Networks. <em>ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2507–2511. <a href="https://doi.org/10.1109/ICASSP.2019.8683170" class="uri">https://doi.org/10.1109/ICASSP.2019.8683170</a></p>
<p>Bakir, V., &amp; McStay, A. (2018). Fake News and The Economy of Emotions. <em>Digital Journalism</em>, 6:2, 154-175. doi:https://doi.org/10.1080/21670811.2017.1345645</p>
<p>Guess, A., Lockett, L., Benjamin, L., Montgomery, J., Nyhan, B., &amp; Reifler, J. (2020). “Fake news” may have limited effects on political participation beyond increasing beliefs in false claims. <em>Harvard Kennedy School (HKS) Misinformation Review</em>. doi:https://doi.org/10.37016/mr-2020-004</p>
<p>Horner, C., Galletta, D., Crawford, J., &amp; Shirsat, A. (2021). Emotions: The Unexplored Fuel of Fake News on Social Media. <em>Journal of Management Information Systems</em>, 38:4, 1039-1066. doi:https://doi.org/10.1080/07421222.2021.1990610</p>
<p>Kohavi, R. (1995). A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. <em>International Joint Conference on Arti</em>, Vol. 14, pp.&nbsp;1137–1145. doi:http://frostiebek.free.fr/docs/Machine%20Learning/validation-1.pdf</p>
<p>Lazer, D. M., Baum, M., Benkler, Y., Berinsky, A., Greenhill, K., Menczer, F., . . . Zittrain, J. (2018). The science of fake news. <em>Science359</em>, 1094-1096. doi:https://doi.org/10.1126/science.aao2998</p>
<p>Liu, B. (2020). Introduction. In B. Liu, <em>Sentiment Analysis Mining Opinions, Sentiments, and Emotions</em> (pp.&nbsp;1-17). Cambridge University Press. doi:https://doi.org/10.1017/9781108639286.002</p>
<p>Swarnkar, N. (2020, May 21). <em>VADER Sentiment Analysis: A Complete Guide, Algo Trading and More</em>. Retrieved from QuantInsti: https://blog.quantinsti.com/vader-sentiment/</p>
<p>Zhang, X., Cao, J., Li, X., Sheng, Q., Zhong, L., &amp; Shu, K. (2021). Mining Dual Emotion for Fake News Detection. <em>Proceedings of the Web Conference 2021</em>, 3465–3476. <a href="https://doi.org/10.1145/3442381.3450004" class="uri">https://doi.org/10.1145/3442381.3450004</a></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>